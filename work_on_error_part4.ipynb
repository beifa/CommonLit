{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "work_on_error_part4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PYW4phrzVXp5uWhz79osUZ_2NZKJNpbg",
      "authorship_tag": "ABX9TyODf/4ombTVOy+c/CJVu3Ip"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgudlyKuktQ-"
      },
      "source": [
        "TODO\n",
        "\n",
        "[github](https://github.com/abhishekkrthakur/commonlit-pairwise-model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4W8bL9H1NZ"
      },
      "source": [
        "from IPython.display import clear_output, Image\n",
        "!pip install transformers\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3XwACKoH1Na"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "import transformers\n",
        "from numpy import random\n",
        "\n",
        "path_tr = '/content/drive/MyDrive/CommonLit/input/train.csv'\n",
        "path_test = '/content/drive/MyDrive/CommonLit/input/test.csv'\n",
        "path_sub = '/content/drive/MyDrive/CommonLit/input/sample_submission.csv'\n",
        "\n",
        "SEED =13\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def create_folds(data, num_splits):\n",
        "    data[\"kfold\"] = -1\n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
        "    data.loc[:, \"bins\"] = pd.cut(\n",
        "        data[\"target\"], bins=num_bins, labels=False\n",
        "    )\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "df = pd.read_csv(path_tr)\n",
        "df = create_folds(df, num_splits=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SCdjqK29fU1"
      },
      "source": [
        "import psutil\n",
        "\n",
        "\n",
        "class CommonlitDataset:\n",
        "    def __init__(self, excerpts, target_dict, error_dict, tokenizer, max_len, num_samples=None):\n",
        "        self.excerpts = excerpts\n",
        "        self.target_dict = target_dict\n",
        "        self.error_dict = error_dict\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.num_samples = num_samples\n",
        "        self.count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.num_samples is None:\n",
        "            return len(self.excerpts)\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if self.num_samples is not None:\n",
        "            self.count += 1\n",
        "            if self.count >= self.num_samples / psutil.cpu_count():\n",
        "                self.count = 0\n",
        "                random.shuffle(self.excerpts)\n",
        "\n",
        "        text1 = str(self.excerpts[item][1])\n",
        "        text2 = str(self.excerpts[item][0])\n",
        "        target = [\n",
        "            self.target_dict[text2],\n",
        "            self.target_dict[text1],\n",
        "        ]\n",
        "\n",
        "        inputs1 = self.tokenizer(text1, max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "        inputs2 = self.tokenizer(text2, max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "\n",
        "        ids1 = inputs1[\"input_ids\"]\n",
        "        mask1 = inputs1[\"attention_mask\"]\n",
        "\n",
        "        ids2 = inputs2[\"input_ids\"]\n",
        "        mask2 = inputs2[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            \"ids1\": torch.tensor(ids1, dtype=torch.long),\n",
        "            \"mask1\": torch.tensor(mask1, dtype=torch.long),\n",
        "            \"ids2\": torch.tensor(ids2, dtype=torch.long),\n",
        "            \"mask2\": torch.tensor(mask2, dtype=torch.long),\n",
        "            \"targets\": torch.tensor(target, dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wzIx8AD9fQx"
      },
      "source": [
        "    args = parse_args()\n",
        "    seed_everything(42)\n",
        "    os.makedirs(args.output_folder, exist_ok=True)\n",
        "    output_path = os.path.join(\n",
        "        args.output_folder,\n",
        "        f\"{args.model.replace('/',':')}__fold_{args.fold}.bin\",\n",
        "    )\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model)\n",
        "    df = pd.read_csv(\"../input/train_folds.csv\")\n",
        "\n",
        "    # base string is excerpt where target is 0 in the dataframe\n",
        "    base_string = df.loc[df.target == 0, \"excerpt\"].values[0]\n",
        "\n",
        "    # create dictionary out of excerpt and target columns from dataframe\n",
        "    target_dict = dict(zip(df.excerpt.values.tolist(), df.target.values.tolist()))\n",
        "    df_train = df[df.kfold != args.fold].reset_index(drop=True)\n",
        "    df_valid = df[df.kfold == args.fold].reset_index(drop=True)\n",
        "    training_pairs = list(itertools.combinations(df_train.excerpt.values.tolist(), 2))\n",
        "\n",
        "    # randomize training_pairs\n",
        "    random.shuffle(training_pairs)\n",
        "    validation_pairs = [(base_string, k) for k in df_valid.excerpt.values.tolist()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NH8unc19fOP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lvkUYA_9fJz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}