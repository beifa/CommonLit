{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "work_on_error_part5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1a6UGXE4hEH73I7Yd7ayKU3yf_eCyD_xv",
      "authorship_tag": "ABX9TyMIiR69ISFAWpChTpl05EQ7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgudlyKuktQ-"
      },
      "source": [
        "TODO\n",
        "\n",
        "[kaggle 22 place solution]('https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257302)\n",
        "\n",
        "[github]('https://github.com/kurupical/commonlit)\n",
        "\n",
        "[inference]('https://www.kaggle.com/kurupical/191-192-202-228-251-253-268-288-278-final?scriptVersionId=69642056)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utm-JHn_UwEu"
      },
      "source": [
        "### worked for me\n",
        "- model ensemble: I thought diversity is the most important thing in this competition.\n",
        "    - At the beginning of the competition, I tested the effectiveness of the ensemble.\n",
        "    - Up to the middle stage, I fixed the model to roberta-large and tried to improve the score.\n",
        "    - At the end, I applied the method to another models. I found that key parameters for this task are {learning_rate, N layers to re-initialize}, so I tuned those parameters for each models.\n",
        "- re-initialization\n",
        "    - This paper (https://arxiv.org/pdf/2006.05987.pdf) shows that fine-tuning with reinitialization last N layers works well.\n",
        "    - Different models have different optimal N. Almost models set N=4~5, gpt2-models set N=6.\n",
        "- LSTM head\n",
        "    - Input BERT's first and last hidden layer into LSTM layer worked well.\n",
        "    - I think first layer represent vocabulary difficulty and last layer represent sentence difficulty. Both are important for inference readbility.\n",
        "- Remove dropout. Improve 0.01~0.02 CV.\n",
        "- gradient clipping. (0.2 or 0.5 works well for me, improve about 0.005 CV)\n",
        "\n",
        "### not worked for me\n",
        "- Input attention matrix to 2D-CNN(like ResNet18 or simple 2DCNN)\n",
        "    - I thought this could represent the complexity of sentences with relative pronouns.\n",
        "- masked 5%~10% vocabulary.\n",
        "- Minimize KLDiv loss to fit distribution.\n",
        "- Scale target to 0~1 and minimize crossentropy loss\n",
        "- \"base\" models excluding mpnet. I got 0.47x CV but Public LB: 0.48x ~ 0.49x.\n",
        "- Stacking using LightGBM.\n",
        "- another models.(result is below table. single CV is well but zero weight for ensemble)\n",
        "- T5. Below notebook achieve 0.47 LB using T5, so I tried but failed.\n",
        "I got only 0.49x(fold 0 only) with learning_rate=1.5e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjh2wZULST-c"
      },
      "source": [
        "configuration for almost all models:\n",
        "```\n",
        "epochs = 4\n",
        "optimizer: AdamW\n",
        "scheduler: linear_schedule_with_warmup(warmup: 5%)\n",
        "lr_bert: 3e-5\n",
        "batch_size: 12\n",
        "gradient clipping: 0.2~0.5\n",
        "reinitialize layers: last 2~6 layers\n",
        "ensemble: Nelder-Mead\n",
        "custom head(finally concat all)\n",
        "    averaging last 4 hidden layer\n",
        "    LSTM head\n",
        "    vocabulary dense\n",
        "hidden_states: (batch_size, vocab_size, bert_hidden_size)\n",
        "  linear_vocab = nn.Sequential(\n",
        "      nn.Linear(bert_hidden_size, 128),\n",
        "      nn.GELU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.GELU()\n",
        "  )\n",
        "  linear_final = nn.Linear(vocab_size * 64, 128)\n",
        "  out = linear_vocab(hidden_states).view(len(input_ids), -1)) # final shape: (batch_size, vocab_size * 64)\n",
        "  out = linear_final(out) # out shape: (batch_size, 128)\n",
        "17 hand-made features\n",
        "    sentence count\n",
        "    average character count in documents\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUpoxLVGWSdY"
      },
      "source": [
        "The main hyperparameters:\n",
        "\n",
        "|nlp_model_name|funnel-large-base|funnel-large|\n",
        "|----|----|----|\n",
        "|dropout|\t0|\t0|\n",
        "batch_size|\t12|\t12|\n",
        "lr_bert|\t2E-05|\t2E-05|\n",
        "lr_fc|\t5E-05|\t5E-05|\n",
        "warmup_ratio|\t0.05|\t0.05|\n",
        "epochs|\t6|\t6|\n",
        "activation|\tGELU|\tGELU|\n",
        "optimizer|\tAdamW|\tAdamW|\n",
        "weight_decay|\t0.1|\t0.1|\n",
        "rnn_module|\tLSTM|\tLSTM|\n",
        "rnn_module_num|\t0|\t1|\n",
        "rnn_hidden_indice|\t(-1, 0)|\t(-1, 0)|\n",
        "linear_vocab_enable|\tFalse|\tTrue|\n",
        "multi_dropout_ratio|\t0.3|\t0.3|\n",
        "multi_dropout_num|\t10|\t10|\n",
        "max_length|\t256|\t256|\n",
        "hidden_stack_enable|\tTrue|\tTrue|\n",
        "reinit_layers|\t4|\t4|\n",
        "gradient_clipping|\t0.2|\t0.2|\n",
        "feature_enable|\tFalse|\tTrue|\n",
        "stochastic_weight_avg|\tFalse|\tFalse|\n",
        "val_check_interval|\t0.05|\t0.05|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4W8bL9H1NZ"
      },
      "source": [
        "from IPython.display import clear_output, Image\n",
        "!pip install transformers\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3XwACKoH1Na"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import itertools\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from numpy import random\n",
        "from torch import nn, optim\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\"\"\"\n",
        "получения информации о запущенных процессах\n",
        "и использовании системы (ЦП, память, диски, сеть, датчики) в Python.\n",
        "\"\"\"\n",
        "import psutil\n",
        "\n",
        "path_tr = '/content/drive/MyDrive/CommonLit/input/train.csv'\n",
        "path_test = '/content/drive/MyDrive/CommonLit/input/test.csv'\n",
        "path_sub = '/content/drive/MyDrive/CommonLit/input/sample_submission.csv'\n",
        "\n",
        "SEED =13\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siMBoJn2Rwvn"
      },
      "source": [
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    experiment_name: str\n",
        "    seed: int = 10\n",
        "    debug: bool = False\n",
        "    fold: int = 0\n",
        "\n",
        "    nlp_model_name: str = \"roberta-base\"\n",
        "    linear_dim: int = 64\n",
        "    linear_vocab_dim_1: int = 64\n",
        "    linear_vocab_dim: int = 16\n",
        "    linear_perplexity_dim: int = 64\n",
        "    linear_final_dim: int = 256\n",
        "    dropout: float = 0\n",
        "    dropout_stack: float = 0\n",
        "    dropout_output_hidden: float = 0\n",
        "    dropout_attn: float = 0\n",
        "    batch_size: int = 32\n",
        "\n",
        "    lr_bert: float = 3e-5\n",
        "    lr_fc: float = 5e-5\n",
        "    lr_rnn: float = 1e-3\n",
        "    lr_tcn: float = 1e-3\n",
        "    lr_cnn: float = 1e-3\n",
        "    warmup_ratio: float = 0.1\n",
        "    training_steps_ratio: float = 1\n",
        "    if debug:\n",
        "        epochs: int = 2\n",
        "        epochs_max: int = 8\n",
        "    else:\n",
        "        epochs: int = 6\n",
        "        epochs_max: int = 6\n",
        "\n",
        "    activation: Any = nn.GELU\n",
        "    optimizer: Any = AdamW\n",
        "    weight_decay: float = 0.1\n",
        "\n",
        "    rnn_module: nn.Module = nn.LSTM\n",
        "    rnn_module_num: int = 0\n",
        "    rnn_module_dropout: float = 0\n",
        "    rnn_module_activation: Any = None\n",
        "    rnn_module_shrink_ratio: float = 0.25\n",
        "    rnn_hidden_indice: Tuple[int] = (-1, 0)\n",
        "    bidirectional: bool = True\n",
        "\n",
        "    tcn_module_enable: bool = False\n",
        "    tcn_module_num: int = 3\n",
        "    tcn_module: nn.Module = TemporalConvNet\n",
        "    tcn_module_kernel_size: int = 4\n",
        "    tcn_module_dropout: float = 0\n",
        "\n",
        "    linear_vocab_enable: bool = False\n",
        "    augmantation_range: Tuple[float, float] = (0, 0)\n",
        "    lr_bert_decay: float = 1\n",
        "\n",
        "    multi_dropout_ratio: float = 0.3\n",
        "    multi_dropout_num: int = 10\n",
        "    fine_tuned_path: str = None\n",
        "\n",
        "    # convnet\n",
        "    cnn_model_name: str = \"resnet18\"\n",
        "    cnn_pretrained: bool = False\n",
        "    self_attention_enable: bool = False\n",
        "\n",
        "    mask_p: float = 0\n",
        "    max_length: int = 256\n",
        "\n",
        "    hidden_stack_enable: bool = False\n",
        "    prep_enable: bool = False\n",
        "    kl_div_enable: bool = False\n",
        "\n",
        "    # reinit\n",
        "    reinit_pooler: bool = True\n",
        "    reinit_layers: int = 4\n",
        "\n",
        "    # pooler\n",
        "    pooler_enable: bool = True\n",
        "\n",
        "    word_axis: bool = False\n",
        "\n",
        "    # conv1d\n",
        "    conv1d_num: int = 1\n",
        "    conv1d_stride: int = 2\n",
        "    conv1d_kernel_size: int = 2\n",
        "\n",
        "    attention_pool_enable: bool = False\n",
        "    conv2d_hidden_channel: int = 32\n",
        "\n",
        "    simple_structure: bool = False\n",
        "    crossentropy: bool = False\n",
        "    crossentropy_min: int = -8\n",
        "    crossentropy_max: int = 4\n",
        "\n",
        "    accumulate_grad_batches: int = 1\n",
        "    gradient_clipping: int = 0.2\n",
        "\n",
        "    dropout_bert: float = 0\n",
        "\n",
        "    feature_enable: bool = False\n",
        "    decoder_only: bool = True\n",
        "\n",
        "    stochastic_weight_avg: bool = False\n",
        "    val_check_interval: float = 0.05\n",
        "\n",
        "    attention_head_enable: bool = False\n",
        "\n",
        "cfg = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTrjgerCRws9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}