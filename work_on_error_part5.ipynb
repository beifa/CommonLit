{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "work_on_error_part5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1e4GKfnEM5LpAfnDsfDWkHwU0BkZZbWlD",
      "authorship_tag": "ABX9TyO79KGUp0B2oGV0h5uljSeM"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgudlyKuktQ-"
      },
      "source": [
        "TODO\n",
        "\n",
        "[kaggle 22 place solution]('https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257302)\n",
        "\n",
        "[github]('https://github.com/kurupical/commonlit)\n",
        "\n",
        "[inference]('https://www.kaggle.com/kurupical/191-192-202-228-251-253-268-288-278-final?scriptVersionId=69642056)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utm-JHn_UwEu"
      },
      "source": [
        "## Description\n",
        "\n",
        "### worked for me\n",
        "- model ensemble: I thought diversity is the most important thing in this competition.\n",
        "    - At the beginning of the competition, I tested the effectiveness of the ensemble.\n",
        "    - Up to the middle stage, I fixed the model to roberta-large and tried to improve the score.\n",
        "    - At the end, I applied the method to another models. I found that key parameters for this task are {learning_rate, N layers to re-initialize}, so I tuned those parameters for each models.\n",
        "- re-initialization\n",
        "    - This paper (https://arxiv.org/pdf/2006.05987.pdf) shows that fine-tuning with reinitialization last N layers works well.\n",
        "    - Different models have different optimal N. Almost models set N=4~5, gpt2-models set N=6.\n",
        "- LSTM head\n",
        "    - Input BERT's first and last hidden layer into LSTM layer worked well.\n",
        "    - I think first layer represent vocabulary difficulty and last layer represent sentence difficulty. Both are important for inference readbility.\n",
        "- Remove dropout. Improve 0.01~0.02 CV.\n",
        "- gradient clipping. (0.2 or 0.5 works well for me, improve about 0.005 CV)\n",
        "\n",
        "### not worked for me\n",
        "- Input attention matrix to 2D-CNN(like ResNet18 or simple 2DCNN)\n",
        "    - I thought this could represent the complexity of sentences with relative pronouns.\n",
        "- masked 5%~10% vocabulary.\n",
        "- Minimize KLDiv loss to fit distribution.\n",
        "- Scale target to 0~1 and minimize crossentropy loss\n",
        "- \"base\" models excluding mpnet. I got 0.47x CV but Public LB: 0.48x ~ 0.49x.\n",
        "- Stacking using LightGBM.\n",
        "- another models.(result is below table. single CV is well but zero weight for ensemble)\n",
        "- T5. Below notebook achieve 0.47 LB using T5, so I tried but failed.\n",
        "I got only 0.49x(fold 0 only) with learning_rate=1.5e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjh2wZULST-c"
      },
      "source": [
        "configuration for almost all models:\n",
        "```\n",
        "epochs = 4\n",
        "optimizer: AdamW\n",
        "scheduler: linear_schedule_with_warmup(warmup: 5%)\n",
        "lr_bert: 3e-5\n",
        "batch_size: 12\n",
        "gradient clipping: 0.2~0.5\n",
        "reinitialize layers: last 2~6 layers\n",
        "ensemble: Nelder-Mead\n",
        "custom head(finally concat all)\n",
        "    averaging last 4 hidden layer\n",
        "    LSTM head\n",
        "    vocabulary dense\n",
        "hidden_states: (batch_size, vocab_size, bert_hidden_size)\n",
        "  linear_vocab = nn.Sequential(\n",
        "      nn.Linear(bert_hidden_size, 128),\n",
        "      nn.GELU(),\n",
        "      nn.Linear(128, 64),\n",
        "      nn.GELU()\n",
        "  )\n",
        "  linear_final = nn.Linear(vocab_size * 64, 128)\n",
        "  out = linear_vocab(hidden_states).view(len(input_ids), -1)) # final shape: (batch_size, vocab_size * 64)\n",
        "  out = linear_final(out) # out shape: (batch_size, 128)\n",
        "17 hand-made features\n",
        "    sentence count\n",
        "    average character count in documents\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUpoxLVGWSdY"
      },
      "source": [
        "The main hyperparameters:\n",
        "\n",
        "|nlp_model_name|funnel-large-base|funnel-large|\n",
        "|----|----|----|\n",
        "|dropout|\t0|\t0|\n",
        "|batch_size|\t12|\t12|\n",
        "|lr_bert|\t2E-05|\t2E-05|\n",
        "|lr_fc|\t5E-05|\t5E-05|\n",
        "|warmup_ratio|\t0.05|\t0.05|\n",
        "|epochs|\t6|\t6|\n",
        "|activation|\tGELU|\tGELU|\n",
        "|optimizer|\tAdamW|\tAdamW|\n",
        "|weight_decay|\t0.1|\t0.1|\n",
        "|rnn_module|\tLSTM|\tLSTM|\n",
        "|rnn_module_num|\t0|\t1|\n",
        "|rnn_hidden_indice|\t(-1, 0)|\t(-1, 0)|\n",
        "|linear_vocab_enable|\tFalse|\tTrue|\n",
        "|multi_dropout_ratio|\t0.3|\t0.3|\n",
        "|multi_dropout_num|\t10|\t10|\n",
        "|max_length|\t256|\t256|\n",
        "|hidden_stack_enable|\tTrue|\tTrue|\n",
        "|reinit_layers|\t4|\t4|\n",
        "|gradient_clipping|\t0.2|\t0.2|\n",
        "|feature_enable|\tFalse|\tTrue|\n",
        "|stochastic_weight_avg|\tFalse|\tFalse|\n",
        "|val_check_interval|\t0.05|\t0.05|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKhFXf_NgRAy"
      },
      "source": [
        "look for https://github.com/kurupical/commonlit/blob/master/exp/exp278.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4W8bL9H1NZ"
      },
      "source": [
        "from IPython.display import clear_output, Image\n",
        "!pip install transformers\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VrF0HxynKym"
      },
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.9\n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n",
        "# !sudo update-alternatives --config python3\n",
        "clear_output()\n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3XwACKoH1Na"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import itertools\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from numpy import random\n",
        "from torch import nn, optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\"\"\"\n",
        "any:Это означает, что можно выполнить любую операцию или вызов метода \n",
        "    для значения типа Any и присвоить его любой переменной\n",
        "\n",
        "optional: Обратите внимание, что это не то же самое, что необязательный\n",
        "          аргумент, который имеет значение по умолчанию.\n",
        "\"\"\"\n",
        "from typing import Any, Optional, List, Tuple\n",
        "\n",
        "\"\"\"\n",
        "получения информации о запущенных процессах\n",
        "и использовании системы (ЦП, память, диски, сеть, датчики) в Python.\n",
        "\"\"\"\n",
        "import psutil\n",
        "\n",
        "path_tr = '/content/drive/MyDrive/CommonLit/input/train.csv'\n",
        "path_test = '/content/drive/MyDrive/CommonLit/input/test.csv'\n",
        "path_sub = '/content/drive/MyDrive/CommonLit/input/sample_submission.csv'\n",
        "\n",
        "SEED =13\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dIZ7PS2kV6r"
      },
      "source": [
        "## dataclass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siMBoJn2Rwvn"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    experiment_name: str\n",
        "    seed: int = 10\n",
        "    debug: bool = False\n",
        "    fold: int = 0\n",
        "\n",
        "    nlp_model_name: str = \"roberta-base\"\n",
        "    linear_dim: int = 64\n",
        "    linear_vocab_dim_1: int = 64\n",
        "    linear_vocab_dim: int = 16\n",
        "    linear_perplexity_dim: int = 64\n",
        "    linear_final_dim: int = 256\n",
        "    dropout: float = 0\n",
        "    dropout_stack: float = 0\n",
        "    dropout_output_hidden: float = 0\n",
        "    dropout_attn: float = 0\n",
        "    batch_size: int = 32\n",
        "\n",
        "    lr_bert: float = 3e-5\n",
        "    lr_fc: float = 5e-5\n",
        "    lr_rnn: float = 1e-3\n",
        "    lr_tcn: float = 1e-3\n",
        "    lr_cnn: float = 1e-3\n",
        "    warmup_ratio: float = 0.1\n",
        "    training_steps_ratio: float = 1\n",
        "    if debug:\n",
        "        epochs: int = 2\n",
        "        epochs_max: int = 8\n",
        "    else:\n",
        "        epochs: int = 6\n",
        "        epochs_max: int = 6\n",
        "\n",
        "    activation: Any = nn.GELU\n",
        "    # optimizer: Any = transformers.AdamW\n",
        "    weight_decay: float = 0.1\n",
        "\n",
        "    rnn_module: nn.Module = nn.LSTM\n",
        "    rnn_module_num: int = 0\n",
        "    rnn_module_dropout: float = 0\n",
        "    rnn_module_activation: Any = None\n",
        "    rnn_module_shrink_ratio: float = 0.25\n",
        "    rnn_hidden_indice: Tuple[int] = (-1, 0)\n",
        "    bidirectional: bool = True\n",
        "\n",
        "    tcn_module_enable: bool = False\n",
        "    tcn_module_num: int = 3\n",
        "    # tcn_module: nn.Module = TemporalConvNet\n",
        "    tcn_module_kernel_size: int = 4\n",
        "    tcn_module_dropout: float = 0\n",
        "\n",
        "    linear_vocab_enable: bool = False\n",
        "    augmantation_range: Tuple[float, float] = (0, 0)\n",
        "    lr_bert_decay: float = 1\n",
        "\n",
        "    multi_dropout_ratio: float = 0.3\n",
        "    multi_dropout_num: int = 10\n",
        "    fine_tuned_path: str = None\n",
        "\n",
        "    # convnet\n",
        "    cnn_model_name: str = \"resnet18\"\n",
        "    cnn_pretrained: bool = False\n",
        "    self_attention_enable: bool = False\n",
        "\n",
        "    mask_p: float = 0\n",
        "    max_length: int = 256\n",
        "\n",
        "    hidden_stack_enable: bool = False\n",
        "    prep_enable: bool = False\n",
        "    kl_div_enable: bool = False\n",
        "\n",
        "    # reinit\n",
        "    reinit_pooler: bool = True\n",
        "    reinit_layers: int = 4\n",
        "\n",
        "    # pooler\n",
        "    pooler_enable: bool = True\n",
        "\n",
        "    word_axis: bool = False\n",
        "\n",
        "    # conv1d\n",
        "    conv1d_num: int = 1\n",
        "    conv1d_stride: int = 2\n",
        "    conv1d_kernel_size: int = 2\n",
        "\n",
        "    attention_pool_enable: bool = False\n",
        "    conv2d_hidden_channel: int = 32\n",
        "\n",
        "    simple_structure: bool = False\n",
        "    crossentropy: bool = False\n",
        "    crossentropy_min: int = -8\n",
        "    crossentropy_max: int = 4\n",
        "\n",
        "    accumulate_grad_batches: int = 1\n",
        "    gradient_clipping: int = 0.2\n",
        "\n",
        "    dropout_bert: float = 0\n",
        "\n",
        "    feature_enable: bool = False\n",
        "    decoder_only: bool = True\n",
        "\n",
        "    stochastic_weight_avg: bool = False\n",
        "    val_check_interval: float = 0.05\n",
        "\n",
        "    attention_head_enable: bool = False\n",
        "\n",
        "cfg = Config('test1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB2tWMiZqz8b",
        "outputId": "ed327e03-e761-44ec-86bd-c9dced16ceec"
      },
      "source": [
        "cfg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config(experiment_name='test1', seed=10, debug=False, fold=0, nlp_model_name='roberta-base', linear_dim=64, linear_vocab_dim_1=64, linear_vocab_dim=16, linear_perplexity_dim=64, linear_final_dim=256, dropout=0, dropout_stack=0, dropout_output_hidden=0, dropout_attn=0, batch_size=32, lr_bert=3e-05, lr_fc=5e-05, lr_rnn=0.001, lr_tcn=0.001, lr_cnn=0.001, warmup_ratio=0.1, training_steps_ratio=1, epochs=6, epochs_max=6, activation=<class 'torch.nn.modules.activation.GELU'>, weight_decay=0.1, rnn_module=<class 'torch.nn.modules.rnn.LSTM'>, rnn_module_num=0, rnn_module_dropout=0, rnn_module_activation=None, rnn_module_shrink_ratio=0.25, rnn_hidden_indice=(-1, 0), bidirectional=True, tcn_module_enable=False, tcn_module_num=3, tcn_module_kernel_size=4, tcn_module_dropout=0, linear_vocab_enable=False, augmantation_range=(0, 0), lr_bert_decay=1, multi_dropout_ratio=0.3, multi_dropout_num=10, fine_tuned_path=None, cnn_model_name='resnet18', cnn_pretrained=False, self_attention_enable=False, mask_p=0, max_length=256, hidden_stack_enable=False, prep_enable=False, kl_div_enable=False, reinit_pooler=True, reinit_layers=4, pooler_enable=True, word_axis=False, conv1d_num=1, conv1d_stride=2, conv1d_kernel_size=2, attention_pool_enable=False, conv2d_hidden_channel=32, simple_structure=False, crossentropy=False, crossentropy_min=-8, crossentropy_max=4, accumulate_grad_batches=1, gradient_clipping=0.2, dropout_bert=0, feature_enable=False, decoder_only=True, stochastic_weight_avg=False, val_check_interval=0.05, attention_head_enable=False)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBILdlwBdOPm"
      },
      "source": [
        "## feature_engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTrjgerCRws9"
      },
      "source": [
        "def total_words(x):\n",
        "    return len(x.split(\" \"))\n",
        "\n",
        "def total_unique_words(x):\n",
        "    return len(np.unique(x.split(\" \")))\n",
        "\n",
        "def total_charactors(x):\n",
        "    x = x.replace(\" \", \"\")\n",
        "    return len(x)\n",
        "\n",
        "def total_sentence(x):\n",
        "    x = x.replace(\"!\", \"[end]\").replace(\"?\", \"[end]\").replace(\".\", \"[end]\")\n",
        "    return len(x.split(\"[end]\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BZCpAshjVZh",
        "outputId": "ac216fa6-0f4e-4c20-9869-60af48131d33"
      },
      "source": [
        "df = pd.read_csv(path_tr)\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'url_legal', 'license', 'excerpt', 'target', 'standard_error'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "xkF1AzqIjZZG",
        "outputId": "c7dc6144-9f70-482d-d9dc-4af4ee2590d6"
      },
      "source": [
        "df_ret = df[[\"id\", \"excerpt\", \"target\", \"standard_error\"]]\n",
        "excerpt = df[\"excerpt\"].values\n",
        "df_ret[\"total_words\"] = [total_words(x) for x in excerpt]\n",
        "df_ret[\"total_unique_words\"] = [total_unique_words(x) for x in excerpt]\n",
        "df_ret[\"total_characters\"] = [total_charactors(x) for x in excerpt]\n",
        "df_ret[\"total_sentence\"] = [total_sentence(x) for x in excerpt]\n",
        "\n",
        "df_ret[\"div_sentence_characters\"] = df_ret[\"total_sentence\"] / df_ret[\"total_characters\"]\n",
        "df_ret[\"div_sentence_words\"] = df_ret[\"total_sentence\"] / df_ret[\"total_words\"]\n",
        "df_ret[\"div_characters_words\"] = df_ret[\"total_characters\"] / df_ret[\"total_words\"]\n",
        "df_ret[\"div_words_unique_words\"] = df_ret[\"total_words\"] / df_ret[\"total_unique_words\"]\n",
        "\n",
        "for i, word in enumerate([\"!\", \"?\", \"(\", \")\", \"'\", '\"', \";\", \".\", \",\"]):\n",
        "    df_ret[f\"count_word_special_{i}\"] = [x.count(word) for x in excerpt]\n",
        "df_ret.fillna(0, inplace=True)\n",
        "\n",
        "df_ret.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>total_words</th>\n",
              "      <th>total_unique_words</th>\n",
              "      <th>total_characters</th>\n",
              "      <th>total_sentence</th>\n",
              "      <th>div_sentence_characters</th>\n",
              "      <th>div_sentence_words</th>\n",
              "      <th>div_characters_words</th>\n",
              "      <th>div_words_unique_words</th>\n",
              "      <th>count_word_special_0</th>\n",
              "      <th>count_word_special_1</th>\n",
              "      <th>count_word_special_2</th>\n",
              "      <th>count_word_special_3</th>\n",
              "      <th>count_word_special_4</th>\n",
              "      <th>count_word_special_5</th>\n",
              "      <th>count_word_special_6</th>\n",
              "      <th>count_word_special_7</th>\n",
              "      <th>count_word_special_8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "      <td>174</td>\n",
              "      <td>112</td>\n",
              "      <td>819</td>\n",
              "      <td>12</td>\n",
              "      <td>0.014652</td>\n",
              "      <td>0.068966</td>\n",
              "      <td>4.706897</td>\n",
              "      <td>1.553571</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "      <td>164</td>\n",
              "      <td>123</td>\n",
              "      <td>774</td>\n",
              "      <td>18</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.109756</td>\n",
              "      <td>4.719512</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b69ac6792</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "      <td>162</td>\n",
              "      <td>124</td>\n",
              "      <td>747</td>\n",
              "      <td>13</td>\n",
              "      <td>0.017403</td>\n",
              "      <td>0.080247</td>\n",
              "      <td>4.611111</td>\n",
              "      <td>1.306452</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dd1000b26</td>\n",
              "      <td>And outside before the palace a great garden w...</td>\n",
              "      <td>-1.054013</td>\n",
              "      <td>0.450007</td>\n",
              "      <td>163</td>\n",
              "      <td>117</td>\n",
              "      <td>747</td>\n",
              "      <td>6</td>\n",
              "      <td>0.008032</td>\n",
              "      <td>0.036810</td>\n",
              "      <td>4.582822</td>\n",
              "      <td>1.393162</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37c1b32fb</td>\n",
              "      <td>Once upon a time there were Three Bears who li...</td>\n",
              "      <td>0.247197</td>\n",
              "      <td>0.510845</td>\n",
              "      <td>147</td>\n",
              "      <td>51</td>\n",
              "      <td>577</td>\n",
              "      <td>6</td>\n",
              "      <td>0.010399</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>3.925170</td>\n",
              "      <td>2.882353</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ... count_word_special_8\n",
              "0  c12129c31  ...                   14\n",
              "1  85aa80a4c  ...                   24\n",
              "2  b69ac6792  ...                   17\n",
              "3  dd1000b26  ...                   23\n",
              "4  37c1b32fb  ...                   13\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urhIgnksJyKT",
        "outputId": "ee7ad7f9-08ef-4988-d185-da8a0d3cd670"
      },
      "source": [
        "cfg.feature_columns = [x for x in df_ret.columns if x not in [\"id\", \"excerpt\", \"target\", \"kfold\", \"standard_error\"]]\n",
        "\"\"\"\n",
        ".mean\n",
        "    array([1.71654905e+02, 1.13895554e+02, 8.01077982e+02, 1.08479181e+01,\n",
        "        1.37249582e-02, 6.32846225e-02, 4.66937398e+00, 1.51519104e+00,\n",
        "        4.57304164e-01, 3.55681016e-01, 3.68031052e-01, 3.68031052e-01,\n",
        "        1.15031757e+00, 2.38884968e+00, 8.71912491e-01, 9.03493296e+00,\n",
        "        1.17314749e+01])\n",
        "\"\"\"\n",
        "cfg.feature_mean = df_ret[cfg.feature_columns].mean().values\n",
        "cfg.feature_std = df_ret[cfg.feature_columns].std().values\n",
        "cfg.feature_columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['total_words',\n",
              " 'total_unique_words',\n",
              " 'total_characters',\n",
              " 'total_sentence',\n",
              " 'div_sentence_characters',\n",
              " 'div_sentence_words',\n",
              " 'div_characters_words',\n",
              " 'div_words_unique_words',\n",
              " 'count_word_special_0',\n",
              " 'count_word_special_1',\n",
              " 'count_word_special_2',\n",
              " 'count_word_special_3',\n",
              " 'count_word_special_4',\n",
              " 'count_word_special_5',\n",
              " 'count_word_special_6',\n",
              " 'count_word_special_7',\n",
              " 'count_word_special_8']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySUBNgrUXAhV"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Bjwo3CRQ96"
      },
      "source": [
        "class CommonLitDataset(Dataset):\n",
        "    \"\"\"   \n",
        "    return:\n",
        "        input_ids_masked\n",
        "        attention_mask\n",
        "        token_type_ids\n",
        "        input_ids\n",
        "\n",
        "        features - array norm maked features\n",
        "        target - target\n",
        "        std - \"standard_error\" from data ori\n",
        "    \"\"\"\n",
        "    def __init__(self, df, tokenizer, cfg, transforms=None):\n",
        "        self.df = df.reset_index()\n",
        "        self.augmentations = transforms\n",
        "        self.cfg = cfg\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "\n",
        "        text_original = row[\"excerpt\"]\n",
        "\n",
        "        text = self.tokenizer(text_original,\n",
        "                              padding=\"max_length\",\n",
        "                              max_length=self.cfg.max_length,\n",
        "                              truncation=True,\n",
        "                              return_tensors=\"pt\",\n",
        "                              return_token_type_ids=True)\n",
        "        input_ids = text[\"input_ids\"][0].detach().cpu().numpy()\n",
        "        input_ids_masked = [x if np.random.random() > self.cfg.mask_p else self.tokenizer.mask_token_id for x in input_ids]\n",
        "        input_ids_masked = torch.LongTensor(input_ids_masked).to(\"cuda\")\n",
        "        attention_mask = text[\"attention_mask\"][0]\n",
        "        token_type_ids = text[\"token_type_ids\"][0]\n",
        "        std = row[\"standard_error\"]\n",
        "\n",
        "        features = ((row[self.cfg.feature_columns].fillna(0).values - self.cfg.feature_mean) / self.cfg.feature_std)\n",
        "        \"\"\"\n",
        "        take currnet (row and - mean(all)) / std(all)\n",
        "        array([ 0.13797008, -0.14786123,  0.17155507,  0.24627862,  0.14901487,\n",
        "                0.21311318,  0.08742476,  0.26920065, -0.39593219, -0.3817358 ,\n",
        "               -0.38798113, -0.38889757, -0.63937729, -0.57226923, -0.66869829,\n",
        "                0.4939904 ,  0.48264299])                \n",
        "        \"\"\"\n",
        "        features = torch.tensor(features, dtype=torch.float)\n",
        "        target = torch.tensor(row[\"target\"], dtype=torch.float)\n",
        "        return input_ids_masked, attention_mask, token_type_ids, input_ids, features, target, std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZMPNznZZow-"
      },
      "source": [
        "## Module&Heads, Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3b5fInuaJ42"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning.utilities import rank_zero_warn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyNig193Zw1I"
      },
      "source": [
        "import gc\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# Applies weight normalization to a parameter in the given module.\n",
        "from torch.nn.utils import weight_norm\n",
        "\"\"\"\n",
        "https://github.com/rwightman/pytorch-image-models#introduction\n",
        "\n",
        "Модели изображений PyTorch (timm) - это набор моделей изображений, слоев, утилит, оптимизаторов,\n",
        "планировщиков, загрузчиков / дополнений данных и эталонных сценариев обучения / проверки, которые\n",
        "призваны объединить широкий спектр моделей SOTA с возможностью воспроизведения обучения ImageNet.\n",
        "полученные результаты.\n",
        "\"\"\"\n",
        "import timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moNq01FJcnz6"
      },
      "source": [
        "### TemporalConvNet\n",
        "[link]('https://web.cse.ohio-state.edu/~wang.77/papers/Pandey-Wang1.icassp19.pdf)\n",
        "\n",
        "\n",
        "Предлагаемая CNN представляет собой архитектуру на основе кодера-декодера с дополнительным временным сверточным модулем.\n",
        "\n",
        "(TCM) вставляется между кодировщиком и декодером.\n",
        "Мы называем эта архитектура - временная сверточная нейронная сеть.(TCNN). \n",
        "\n",
        "Кодер в TCNN создает низкоразмерное представление зашумленного входного кадра. TCM использует\n",
        "причинные и расширенные сверточные слои для использования кодировщика\n",
        "вывод текущего и предыдущего кадров. Декодер использует\n",
        "выход TCM для восстановления улучшенного кадра. Предлагаемая модель обучается не зависящим от динамика и шума.\n",
        "Экспериментальные результаты показывают, что предложенный\n",
        "модель дает стабильно лучшие результаты улучшения, чем\n",
        "современная сверточная рекуррентная модель в реальном времени.\n",
        "Более того, поскольку модель является полностью сверточной, в ней много меньше обучаемых параметров, чем в более ранних моделях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3awfO1GRQvp"
      },
      "source": [
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=(kernel_size-1)*dilation,\n",
        "                                           dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=(kernel_size-1)*dilation,\n",
        "                                           dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1, padding=(kernel_size-1)*dilation) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "   \n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]  \n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B27QxkuxgdYq"
      },
      "source": [
        "\"\"\"\n",
        "cfg.max_length, cfg.tcn_module_num, cfg.tcn_module_kernel_size\n",
        ">>> (256, 3, 4)\n",
        "\n",
        "in model:\n",
        "    num_levels = 3\n",
        "    in_channels=256\n",
        "    out_channels=256\n",
        "\"\"\"\n",
        "\n",
        "tcn_model = TemporalConvNet(\n",
        "    num_inputs=256,\n",
        "    num_channels=[256] * 3,\n",
        "    kernel_size=cfg.tcn_module_kernel_size,\n",
        "    dropout=cfg.tcn_module_dropout\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayBzWSYRgdWI",
        "outputId": "e726e2ac-c172-467a-b274-3287f38798df"
      },
      "source": [
        "tcn_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TemporalConvNet(\n",
              "  (network): Sequential(\n",
              "    (0): TemporalBlock(\n",
              "      (conv1): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,))\n",
              "      (chomp1): Chomp1d()\n",
              "      (relu1): ReLU()\n",
              "      (dropout1): Dropout(p=0, inplace=False)\n",
              "      (conv2): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,))\n",
              "      (chomp2): Chomp1d()\n",
              "      (relu2): ReLU()\n",
              "      (dropout2): Dropout(p=0, inplace=False)\n",
              "      (net): Sequential(\n",
              "        (0): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,))\n",
              "        (1): Chomp1d()\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0, inplace=False)\n",
              "        (4): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,))\n",
              "        (5): Chomp1d()\n",
              "        (6): ReLU()\n",
              "        (7): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (1): TemporalBlock(\n",
              "      (conv1): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(6,), dilation=(2,))\n",
              "      (chomp1): Chomp1d()\n",
              "      (relu1): ReLU()\n",
              "      (dropout1): Dropout(p=0, inplace=False)\n",
              "      (conv2): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(6,), dilation=(2,))\n",
              "      (chomp2): Chomp1d()\n",
              "      (relu2): ReLU()\n",
              "      (dropout2): Dropout(p=0, inplace=False)\n",
              "      (net): Sequential(\n",
              "        (0): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(6,), dilation=(2,))\n",
              "        (1): Chomp1d()\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0, inplace=False)\n",
              "        (4): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(6,), dilation=(2,))\n",
              "        (5): Chomp1d()\n",
              "        (6): ReLU()\n",
              "        (7): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (2): TemporalBlock(\n",
              "      (conv1): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(12,), dilation=(4,))\n",
              "      (chomp1): Chomp1d()\n",
              "      (relu1): ReLU()\n",
              "      (dropout1): Dropout(p=0, inplace=False)\n",
              "      (conv2): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(12,), dilation=(4,))\n",
              "      (chomp2): Chomp1d()\n",
              "      (relu2): ReLU()\n",
              "      (dropout2): Dropout(p=0, inplace=False)\n",
              "      (net): Sequential(\n",
              "        (0): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(12,), dilation=(4,))\n",
              "        (1): Chomp1d()\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0, inplace=False)\n",
              "        (4): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(12,), dilation=(4,))\n",
              "        (5): Chomp1d()\n",
              "        (6): ReLU()\n",
              "        (7): Dropout(p=0, inplace=False)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usTDIQ8Cm41-"
      },
      "source": [
        "###modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmj-g5X_GVC"
      },
      "source": [
        "#### linear vocab enable(линейный словарь)\n",
        "\n",
        "three variant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ALM6ffZgdTt"
      },
      "source": [
        "# for example input_size = 768 this out model hidden_size\n",
        "linear_vocab = nn.Sequential(\n",
        "    nn.Linear(768, 64),\n",
        "    nn.Dropout(0),\n",
        "    cfg.activation(),\n",
        "    nn.Linear(64, 16),\n",
        "    nn.Dropout(0),\n",
        "    cfg.activation()\n",
        ")\n",
        "# if \"large-base\"\n",
        "linear_vocab_final = nn.Sequential(\n",
        "    nn.Linear(16 * 256 // 4, 256),\n",
        "    # nn.BatchNorm1d(cfg.linear_final_dim),\n",
        "    cfg.activation(),\n",
        "    nn.Dropout(0)\n",
        ")\n",
        "# else\n",
        "linear_vocab_final = nn.Sequential(\n",
        "    nn.Linear(16 * 256 // 4, 256),\n",
        "    # nn.BatchNorm1d(cfg.linear_final_dim),\n",
        "    cfg.activation(),\n",
        "    nn.Dropout(0)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCQVHMWdF-n0"
      },
      "source": [
        "#### attention_enable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpsawkoCKgku"
      },
      "source": [
        "##### Simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZKBp_1jKi1i"
      },
      "source": [
        "convnet = nn.Sequential(\n",
        "    nn.Conv2d(bert.config.num_hidden_layers * bert.config.num_attention_heads,\n",
        "              cfg.conv2d_hidden_channel, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(cfg.conv2d_hidden_channel, 1, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
        "    nn.ReLU(),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "convnet.num_features = 256 ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyClZDa6LzMO"
      },
      "source": [
        "#####TIMM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LVhUe4sI6_B"
      },
      "source": [
        "!pip install timm --quiet\n",
        "clear_output()\n",
        "import timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfj4iSBaJZ8K",
        "outputId": "23a0ec51-b7d5-462d-d641-3896288a9f1c"
      },
      "source": [
        "timm.list_models()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adv_inception_v3',\n",
              " 'bat_resnext26ts',\n",
              " 'botnet26t_256',\n",
              " 'botnet50ts_256',\n",
              " 'cait_m36_384',\n",
              " 'cait_m48_448',\n",
              " 'cait_s24_224',\n",
              " 'cait_s24_384',\n",
              " 'cait_s36_384',\n",
              " 'cait_xs24_384',\n",
              " 'cait_xxs24_224',\n",
              " 'cait_xxs24_384',\n",
              " 'cait_xxs36_224',\n",
              " 'cait_xxs36_384',\n",
              " 'coat_lite_mini',\n",
              " 'coat_lite_small',\n",
              " 'coat_lite_tiny',\n",
              " 'coat_mini',\n",
              " 'coat_tiny',\n",
              " 'convit_base',\n",
              " 'convit_small',\n",
              " 'convit_tiny',\n",
              " 'cspdarknet53',\n",
              " 'cspdarknet53_iabn',\n",
              " 'cspresnet50',\n",
              " 'cspresnet50d',\n",
              " 'cspresnet50w',\n",
              " 'cspresnext50',\n",
              " 'cspresnext50_iabn',\n",
              " 'darknet53',\n",
              " 'deit_base_distilled_patch16_224',\n",
              " 'deit_base_distilled_patch16_384',\n",
              " 'deit_base_patch16_224',\n",
              " 'deit_base_patch16_384',\n",
              " 'deit_small_distilled_patch16_224',\n",
              " 'deit_small_patch16_224',\n",
              " 'deit_tiny_distilled_patch16_224',\n",
              " 'deit_tiny_patch16_224',\n",
              " 'densenet121',\n",
              " 'densenet121d',\n",
              " 'densenet161',\n",
              " 'densenet169',\n",
              " 'densenet201',\n",
              " 'densenet264',\n",
              " 'densenet264d_iabn',\n",
              " 'densenetblur121d',\n",
              " 'dla34',\n",
              " 'dla46_c',\n",
              " 'dla46x_c',\n",
              " 'dla60',\n",
              " 'dla60_res2net',\n",
              " 'dla60_res2next',\n",
              " 'dla60x',\n",
              " 'dla60x_c',\n",
              " 'dla102',\n",
              " 'dla102x',\n",
              " 'dla102x2',\n",
              " 'dla169',\n",
              " 'dm_nfnet_f0',\n",
              " 'dm_nfnet_f1',\n",
              " 'dm_nfnet_f2',\n",
              " 'dm_nfnet_f3',\n",
              " 'dm_nfnet_f4',\n",
              " 'dm_nfnet_f5',\n",
              " 'dm_nfnet_f6',\n",
              " 'dpn68',\n",
              " 'dpn68b',\n",
              " 'dpn92',\n",
              " 'dpn98',\n",
              " 'dpn107',\n",
              " 'dpn131',\n",
              " 'eca_botnext26ts_256',\n",
              " 'eca_efficientnet_b0',\n",
              " 'eca_halonext26ts',\n",
              " 'eca_lambda_resnext26ts',\n",
              " 'eca_nfnet_l0',\n",
              " 'eca_nfnet_l1',\n",
              " 'eca_nfnet_l2',\n",
              " 'eca_nfnet_l3',\n",
              " 'eca_swinnext26ts_256',\n",
              " 'eca_vovnet39b',\n",
              " 'ecaresnet26t',\n",
              " 'ecaresnet50d',\n",
              " 'ecaresnet50d_pruned',\n",
              " 'ecaresnet50t',\n",
              " 'ecaresnet101d',\n",
              " 'ecaresnet101d_pruned',\n",
              " 'ecaresnet200d',\n",
              " 'ecaresnet269d',\n",
              " 'ecaresnetlight',\n",
              " 'ecaresnext26t_32x4d',\n",
              " 'ecaresnext50t_32x4d',\n",
              " 'efficientnet_b0',\n",
              " 'efficientnet_b1',\n",
              " 'efficientnet_b1_pruned',\n",
              " 'efficientnet_b2',\n",
              " 'efficientnet_b2_pruned',\n",
              " 'efficientnet_b2a',\n",
              " 'efficientnet_b3',\n",
              " 'efficientnet_b3_pruned',\n",
              " 'efficientnet_b3a',\n",
              " 'efficientnet_b4',\n",
              " 'efficientnet_b5',\n",
              " 'efficientnet_b6',\n",
              " 'efficientnet_b7',\n",
              " 'efficientnet_b8',\n",
              " 'efficientnet_cc_b0_4e',\n",
              " 'efficientnet_cc_b0_8e',\n",
              " 'efficientnet_cc_b1_8e',\n",
              " 'efficientnet_el',\n",
              " 'efficientnet_el_pruned',\n",
              " 'efficientnet_em',\n",
              " 'efficientnet_es',\n",
              " 'efficientnet_es_pruned',\n",
              " 'efficientnet_l2',\n",
              " 'efficientnet_lite0',\n",
              " 'efficientnet_lite1',\n",
              " 'efficientnet_lite2',\n",
              " 'efficientnet_lite3',\n",
              " 'efficientnet_lite4',\n",
              " 'efficientnetv2_l',\n",
              " 'efficientnetv2_m',\n",
              " 'efficientnetv2_rw_m',\n",
              " 'efficientnetv2_rw_s',\n",
              " 'efficientnetv2_s',\n",
              " 'ens_adv_inception_resnet_v2',\n",
              " 'ese_vovnet19b_dw',\n",
              " 'ese_vovnet19b_slim',\n",
              " 'ese_vovnet19b_slim_dw',\n",
              " 'ese_vovnet39b',\n",
              " 'ese_vovnet39b_evos',\n",
              " 'ese_vovnet57b',\n",
              " 'ese_vovnet99b',\n",
              " 'ese_vovnet99b_iabn',\n",
              " 'fbnetc_100',\n",
              " 'fbnetv3_b',\n",
              " 'fbnetv3_d',\n",
              " 'fbnetv3_g',\n",
              " 'gc_efficientnet_b0',\n",
              " 'gcresnet50t',\n",
              " 'gcresnext26ts',\n",
              " 'geresnet50t',\n",
              " 'gernet_l',\n",
              " 'gernet_m',\n",
              " 'gernet_s',\n",
              " 'ghostnet_050',\n",
              " 'ghostnet_100',\n",
              " 'ghostnet_130',\n",
              " 'gluon_inception_v3',\n",
              " 'gluon_resnet18_v1b',\n",
              " 'gluon_resnet34_v1b',\n",
              " 'gluon_resnet50_v1b',\n",
              " 'gluon_resnet50_v1c',\n",
              " 'gluon_resnet50_v1d',\n",
              " 'gluon_resnet50_v1s',\n",
              " 'gluon_resnet101_v1b',\n",
              " 'gluon_resnet101_v1c',\n",
              " 'gluon_resnet101_v1d',\n",
              " 'gluon_resnet101_v1s',\n",
              " 'gluon_resnet152_v1b',\n",
              " 'gluon_resnet152_v1c',\n",
              " 'gluon_resnet152_v1d',\n",
              " 'gluon_resnet152_v1s',\n",
              " 'gluon_resnext50_32x4d',\n",
              " 'gluon_resnext101_32x4d',\n",
              " 'gluon_resnext101_64x4d',\n",
              " 'gluon_senet154',\n",
              " 'gluon_seresnext50_32x4d',\n",
              " 'gluon_seresnext101_32x4d',\n",
              " 'gluon_seresnext101_64x4d',\n",
              " 'gluon_xception65',\n",
              " 'gmixer_12_224',\n",
              " 'gmixer_24_224',\n",
              " 'gmlp_b16_224',\n",
              " 'gmlp_s16_224',\n",
              " 'gmlp_ti16_224',\n",
              " 'halonet26t',\n",
              " 'halonet50ts',\n",
              " 'halonet_h1',\n",
              " 'halonet_h1_c4c5',\n",
              " 'hardcorenas_a',\n",
              " 'hardcorenas_b',\n",
              " 'hardcorenas_c',\n",
              " 'hardcorenas_d',\n",
              " 'hardcorenas_e',\n",
              " 'hardcorenas_f',\n",
              " 'hrnet_w18',\n",
              " 'hrnet_w18_small',\n",
              " 'hrnet_w18_small_v2',\n",
              " 'hrnet_w30',\n",
              " 'hrnet_w32',\n",
              " 'hrnet_w40',\n",
              " 'hrnet_w44',\n",
              " 'hrnet_w48',\n",
              " 'hrnet_w64',\n",
              " 'ig_resnext101_32x8d',\n",
              " 'ig_resnext101_32x16d',\n",
              " 'ig_resnext101_32x32d',\n",
              " 'ig_resnext101_32x48d',\n",
              " 'inception_resnet_v2',\n",
              " 'inception_v3',\n",
              " 'inception_v4',\n",
              " 'lambda_resnet26t',\n",
              " 'lambda_resnet50t',\n",
              " 'legacy_senet154',\n",
              " 'legacy_seresnet18',\n",
              " 'legacy_seresnet34',\n",
              " 'legacy_seresnet50',\n",
              " 'legacy_seresnet101',\n",
              " 'legacy_seresnet152',\n",
              " 'legacy_seresnext26_32x4d',\n",
              " 'legacy_seresnext50_32x4d',\n",
              " 'legacy_seresnext101_32x4d',\n",
              " 'levit_128',\n",
              " 'levit_128s',\n",
              " 'levit_192',\n",
              " 'levit_256',\n",
              " 'levit_384',\n",
              " 'mixer_b16_224',\n",
              " 'mixer_b16_224_in21k',\n",
              " 'mixer_b16_224_miil',\n",
              " 'mixer_b16_224_miil_in21k',\n",
              " 'mixer_b32_224',\n",
              " 'mixer_l16_224',\n",
              " 'mixer_l16_224_in21k',\n",
              " 'mixer_l32_224',\n",
              " 'mixer_s16_224',\n",
              " 'mixer_s32_224',\n",
              " 'mixnet_l',\n",
              " 'mixnet_m',\n",
              " 'mixnet_s',\n",
              " 'mixnet_xl',\n",
              " 'mixnet_xxl',\n",
              " 'mnasnet_050',\n",
              " 'mnasnet_075',\n",
              " 'mnasnet_100',\n",
              " 'mnasnet_140',\n",
              " 'mnasnet_a1',\n",
              " 'mnasnet_b1',\n",
              " 'mnasnet_small',\n",
              " 'mobilenetv2_100',\n",
              " 'mobilenetv2_110d',\n",
              " 'mobilenetv2_120d',\n",
              " 'mobilenetv2_140',\n",
              " 'mobilenetv3_large_075',\n",
              " 'mobilenetv3_large_100',\n",
              " 'mobilenetv3_large_100_miil',\n",
              " 'mobilenetv3_large_100_miil_in21k',\n",
              " 'mobilenetv3_rw',\n",
              " 'mobilenetv3_small_075',\n",
              " 'mobilenetv3_small_100',\n",
              " 'nasnetalarge',\n",
              " 'nf_ecaresnet26',\n",
              " 'nf_ecaresnet50',\n",
              " 'nf_ecaresnet101',\n",
              " 'nf_regnet_b0',\n",
              " 'nf_regnet_b1',\n",
              " 'nf_regnet_b2',\n",
              " 'nf_regnet_b3',\n",
              " 'nf_regnet_b4',\n",
              " 'nf_regnet_b5',\n",
              " 'nf_resnet26',\n",
              " 'nf_resnet50',\n",
              " 'nf_resnet101',\n",
              " 'nf_seresnet26',\n",
              " 'nf_seresnet50',\n",
              " 'nf_seresnet101',\n",
              " 'nfnet_f0',\n",
              " 'nfnet_f0s',\n",
              " 'nfnet_f1',\n",
              " 'nfnet_f1s',\n",
              " 'nfnet_f2',\n",
              " 'nfnet_f2s',\n",
              " 'nfnet_f3',\n",
              " 'nfnet_f3s',\n",
              " 'nfnet_f4',\n",
              " 'nfnet_f4s',\n",
              " 'nfnet_f5',\n",
              " 'nfnet_f5s',\n",
              " 'nfnet_f6',\n",
              " 'nfnet_f6s',\n",
              " 'nfnet_f7',\n",
              " 'nfnet_f7s',\n",
              " 'nfnet_l0',\n",
              " 'pit_b_224',\n",
              " 'pit_b_distilled_224',\n",
              " 'pit_s_224',\n",
              " 'pit_s_distilled_224',\n",
              " 'pit_ti_224',\n",
              " 'pit_ti_distilled_224',\n",
              " 'pit_xs_224',\n",
              " 'pit_xs_distilled_224',\n",
              " 'pnasnet5large',\n",
              " 'rednet26t',\n",
              " 'rednet50ts',\n",
              " 'regnetx_002',\n",
              " 'regnetx_004',\n",
              " 'regnetx_006',\n",
              " 'regnetx_008',\n",
              " 'regnetx_016',\n",
              " 'regnetx_032',\n",
              " 'regnetx_040',\n",
              " 'regnetx_064',\n",
              " 'regnetx_080',\n",
              " 'regnetx_120',\n",
              " 'regnetx_160',\n",
              " 'regnetx_320',\n",
              " 'regnety_002',\n",
              " 'regnety_004',\n",
              " 'regnety_006',\n",
              " 'regnety_008',\n",
              " 'regnety_016',\n",
              " 'regnety_032',\n",
              " 'regnety_040',\n",
              " 'regnety_064',\n",
              " 'regnety_080',\n",
              " 'regnety_120',\n",
              " 'regnety_160',\n",
              " 'regnety_320',\n",
              " 'repvgg_a2',\n",
              " 'repvgg_b0',\n",
              " 'repvgg_b1',\n",
              " 'repvgg_b1g4',\n",
              " 'repvgg_b2',\n",
              " 'repvgg_b2g4',\n",
              " 'repvgg_b3',\n",
              " 'repvgg_b3g4',\n",
              " 'res2net50_14w_8s',\n",
              " 'res2net50_26w_4s',\n",
              " 'res2net50_26w_6s',\n",
              " 'res2net50_26w_8s',\n",
              " 'res2net50_48w_2s',\n",
              " 'res2net101_26w_4s',\n",
              " 'res2next50',\n",
              " 'resmlp_12_224',\n",
              " 'resmlp_12_distilled_224',\n",
              " 'resmlp_24_224',\n",
              " 'resmlp_24_distilled_224',\n",
              " 'resmlp_36_224',\n",
              " 'resmlp_36_distilled_224',\n",
              " 'resmlp_big_24_224',\n",
              " 'resmlp_big_24_224_in22ft1k',\n",
              " 'resmlp_big_24_distilled_224',\n",
              " 'resnest14d',\n",
              " 'resnest26d',\n",
              " 'resnest50d',\n",
              " 'resnest50d_1s4x24d',\n",
              " 'resnest50d_4s2x40d',\n",
              " 'resnest101e',\n",
              " 'resnest200e',\n",
              " 'resnest269e',\n",
              " 'resnet18',\n",
              " 'resnet18d',\n",
              " 'resnet26',\n",
              " 'resnet26d',\n",
              " 'resnet26t',\n",
              " 'resnet34',\n",
              " 'resnet34d',\n",
              " 'resnet50',\n",
              " 'resnet50d',\n",
              " 'resnet50t',\n",
              " 'resnet51q',\n",
              " 'resnet61q',\n",
              " 'resnet101',\n",
              " 'resnet101d',\n",
              " 'resnet152',\n",
              " 'resnet152d',\n",
              " 'resnet200',\n",
              " 'resnet200d',\n",
              " 'resnetblur18',\n",
              " 'resnetblur50',\n",
              " 'resnetrs50',\n",
              " 'resnetrs101',\n",
              " 'resnetrs152',\n",
              " 'resnetrs200',\n",
              " 'resnetrs270',\n",
              " 'resnetrs350',\n",
              " 'resnetrs420',\n",
              " 'resnetv2_50',\n",
              " 'resnetv2_50d',\n",
              " 'resnetv2_50t',\n",
              " 'resnetv2_50x1_bit_distilled',\n",
              " 'resnetv2_50x1_bitm',\n",
              " 'resnetv2_50x1_bitm_in21k',\n",
              " 'resnetv2_50x3_bitm',\n",
              " 'resnetv2_50x3_bitm_in21k',\n",
              " 'resnetv2_101',\n",
              " 'resnetv2_101d',\n",
              " 'resnetv2_101x1_bitm',\n",
              " 'resnetv2_101x1_bitm_in21k',\n",
              " 'resnetv2_101x3_bitm',\n",
              " 'resnetv2_101x3_bitm_in21k',\n",
              " 'resnetv2_152',\n",
              " 'resnetv2_152d',\n",
              " 'resnetv2_152x2_bit_teacher',\n",
              " 'resnetv2_152x2_bit_teacher_384',\n",
              " 'resnetv2_152x2_bitm',\n",
              " 'resnetv2_152x2_bitm_in21k',\n",
              " 'resnetv2_152x4_bitm',\n",
              " 'resnetv2_152x4_bitm_in21k',\n",
              " 'resnext50_32x4d',\n",
              " 'resnext50d_32x4d',\n",
              " 'resnext101_32x4d',\n",
              " 'resnext101_32x8d',\n",
              " 'resnext101_64x4d',\n",
              " 'rexnet_100',\n",
              " 'rexnet_130',\n",
              " 'rexnet_150',\n",
              " 'rexnet_200',\n",
              " 'rexnetr_100',\n",
              " 'rexnetr_130',\n",
              " 'rexnetr_150',\n",
              " 'rexnetr_200',\n",
              " 'selecsls42',\n",
              " 'selecsls42b',\n",
              " 'selecsls60',\n",
              " 'selecsls60b',\n",
              " 'selecsls84',\n",
              " 'semnasnet_050',\n",
              " 'semnasnet_075',\n",
              " 'semnasnet_100',\n",
              " 'semnasnet_140',\n",
              " 'senet154',\n",
              " 'seresnet18',\n",
              " 'seresnet34',\n",
              " 'seresnet50',\n",
              " 'seresnet50t',\n",
              " 'seresnet101',\n",
              " 'seresnet152',\n",
              " 'seresnet152d',\n",
              " 'seresnet200d',\n",
              " 'seresnet269d',\n",
              " 'seresnext26d_32x4d',\n",
              " 'seresnext26t_32x4d',\n",
              " 'seresnext26tn_32x4d',\n",
              " 'seresnext50_32x4d',\n",
              " 'seresnext101_32x4d',\n",
              " 'seresnext101_32x8d',\n",
              " 'skresnet18',\n",
              " 'skresnet34',\n",
              " 'skresnet50',\n",
              " 'skresnet50d',\n",
              " 'skresnext50_32x4d',\n",
              " 'spnasnet_100',\n",
              " 'ssl_resnet18',\n",
              " 'ssl_resnet50',\n",
              " 'ssl_resnext50_32x4d',\n",
              " 'ssl_resnext101_32x4d',\n",
              " 'ssl_resnext101_32x8d',\n",
              " 'ssl_resnext101_32x16d',\n",
              " 'swin_base_patch4_window7_224',\n",
              " 'swin_base_patch4_window7_224_in22k',\n",
              " 'swin_base_patch4_window12_384',\n",
              " 'swin_base_patch4_window12_384_in22k',\n",
              " 'swin_large_patch4_window7_224',\n",
              " 'swin_large_patch4_window7_224_in22k',\n",
              " 'swin_large_patch4_window12_384',\n",
              " 'swin_large_patch4_window12_384_in22k',\n",
              " 'swin_small_patch4_window7_224',\n",
              " 'swin_tiny_patch4_window7_224',\n",
              " 'swinnet26t_256',\n",
              " 'swinnet50ts_256',\n",
              " 'swsl_resnet18',\n",
              " 'swsl_resnet50',\n",
              " 'swsl_resnext50_32x4d',\n",
              " 'swsl_resnext101_32x4d',\n",
              " 'swsl_resnext101_32x8d',\n",
              " 'swsl_resnext101_32x16d',\n",
              " 'tf_efficientnet_b0',\n",
              " 'tf_efficientnet_b0_ap',\n",
              " 'tf_efficientnet_b0_ns',\n",
              " 'tf_efficientnet_b1',\n",
              " 'tf_efficientnet_b1_ap',\n",
              " 'tf_efficientnet_b1_ns',\n",
              " 'tf_efficientnet_b2',\n",
              " 'tf_efficientnet_b2_ap',\n",
              " 'tf_efficientnet_b2_ns',\n",
              " 'tf_efficientnet_b3',\n",
              " 'tf_efficientnet_b3_ap',\n",
              " 'tf_efficientnet_b3_ns',\n",
              " 'tf_efficientnet_b4',\n",
              " 'tf_efficientnet_b4_ap',\n",
              " 'tf_efficientnet_b4_ns',\n",
              " 'tf_efficientnet_b5',\n",
              " 'tf_efficientnet_b5_ap',\n",
              " 'tf_efficientnet_b5_ns',\n",
              " 'tf_efficientnet_b6',\n",
              " 'tf_efficientnet_b6_ap',\n",
              " 'tf_efficientnet_b6_ns',\n",
              " 'tf_efficientnet_b7',\n",
              " 'tf_efficientnet_b7_ap',\n",
              " 'tf_efficientnet_b7_ns',\n",
              " 'tf_efficientnet_b8',\n",
              " 'tf_efficientnet_b8_ap',\n",
              " 'tf_efficientnet_cc_b0_4e',\n",
              " 'tf_efficientnet_cc_b0_8e',\n",
              " 'tf_efficientnet_cc_b1_8e',\n",
              " 'tf_efficientnet_el',\n",
              " 'tf_efficientnet_em',\n",
              " 'tf_efficientnet_es',\n",
              " 'tf_efficientnet_l2_ns',\n",
              " 'tf_efficientnet_l2_ns_475',\n",
              " 'tf_efficientnet_lite0',\n",
              " 'tf_efficientnet_lite1',\n",
              " 'tf_efficientnet_lite2',\n",
              " 'tf_efficientnet_lite3',\n",
              " 'tf_efficientnet_lite4',\n",
              " 'tf_efficientnetv2_b0',\n",
              " 'tf_efficientnetv2_b1',\n",
              " 'tf_efficientnetv2_b2',\n",
              " 'tf_efficientnetv2_b3',\n",
              " 'tf_efficientnetv2_l',\n",
              " 'tf_efficientnetv2_l_in21ft1k',\n",
              " 'tf_efficientnetv2_l_in21k',\n",
              " 'tf_efficientnetv2_m',\n",
              " 'tf_efficientnetv2_m_in21ft1k',\n",
              " 'tf_efficientnetv2_m_in21k',\n",
              " 'tf_efficientnetv2_s',\n",
              " 'tf_efficientnetv2_s_in21ft1k',\n",
              " 'tf_efficientnetv2_s_in21k',\n",
              " 'tf_inception_v3',\n",
              " 'tf_mixnet_l',\n",
              " 'tf_mixnet_m',\n",
              " 'tf_mixnet_s',\n",
              " 'tf_mobilenetv3_large_075',\n",
              " 'tf_mobilenetv3_large_100',\n",
              " 'tf_mobilenetv3_large_minimal_100',\n",
              " 'tf_mobilenetv3_small_075',\n",
              " 'tf_mobilenetv3_small_100',\n",
              " 'tf_mobilenetv3_small_minimal_100',\n",
              " 'tnt_b_patch16_224',\n",
              " 'tnt_s_patch16_224',\n",
              " 'tresnet_l',\n",
              " 'tresnet_l_448',\n",
              " 'tresnet_m',\n",
              " 'tresnet_m_448',\n",
              " 'tresnet_m_miil_in21k',\n",
              " 'tresnet_xl',\n",
              " 'tresnet_xl_448',\n",
              " 'tv_densenet121',\n",
              " 'tv_resnet34',\n",
              " 'tv_resnet50',\n",
              " 'tv_resnet101',\n",
              " 'tv_resnet152',\n",
              " 'tv_resnext50_32x4d',\n",
              " 'twins_pcpvt_base',\n",
              " 'twins_pcpvt_large',\n",
              " 'twins_pcpvt_small',\n",
              " 'twins_svt_base',\n",
              " 'twins_svt_large',\n",
              " 'twins_svt_small',\n",
              " 'vgg11',\n",
              " 'vgg11_bn',\n",
              " 'vgg13',\n",
              " 'vgg13_bn',\n",
              " 'vgg16',\n",
              " 'vgg16_bn',\n",
              " 'vgg19',\n",
              " 'vgg19_bn',\n",
              " 'visformer_small',\n",
              " 'visformer_tiny',\n",
              " 'vit_base_patch16_224',\n",
              " 'vit_base_patch16_224_in21k',\n",
              " 'vit_base_patch16_224_miil',\n",
              " 'vit_base_patch16_224_miil_in21k',\n",
              " 'vit_base_patch16_384',\n",
              " 'vit_base_patch32_224',\n",
              " 'vit_base_patch32_224_in21k',\n",
              " 'vit_base_patch32_384',\n",
              " 'vit_base_r26_s32_224',\n",
              " 'vit_base_r50_s16_224',\n",
              " 'vit_base_r50_s16_224_in21k',\n",
              " 'vit_base_r50_s16_384',\n",
              " 'vit_base_resnet26d_224',\n",
              " 'vit_base_resnet50_224_in21k',\n",
              " 'vit_base_resnet50_384',\n",
              " 'vit_base_resnet50d_224',\n",
              " 'vit_huge_patch14_224_in21k',\n",
              " 'vit_large_patch16_224',\n",
              " 'vit_large_patch16_224_in21k',\n",
              " 'vit_large_patch16_384',\n",
              " 'vit_large_patch32_224',\n",
              " 'vit_large_patch32_224_in21k',\n",
              " 'vit_large_patch32_384',\n",
              " 'vit_large_r50_s32_224',\n",
              " 'vit_large_r50_s32_224_in21k',\n",
              " 'vit_large_r50_s32_384',\n",
              " 'vit_small_patch16_224',\n",
              " 'vit_small_patch16_224_in21k',\n",
              " 'vit_small_patch16_384',\n",
              " 'vit_small_patch32_224',\n",
              " 'vit_small_patch32_224_in21k',\n",
              " 'vit_small_patch32_384',\n",
              " 'vit_small_r26_s32_224',\n",
              " 'vit_small_r26_s32_224_in21k',\n",
              " 'vit_small_r26_s32_384',\n",
              " 'vit_small_resnet26d_224',\n",
              " 'vit_small_resnet50d_s16_224',\n",
              " 'vit_tiny_patch16_224',\n",
              " 'vit_tiny_patch16_224_in21k',\n",
              " 'vit_tiny_patch16_384',\n",
              " 'vit_tiny_r_s16_p8_224',\n",
              " 'vit_tiny_r_s16_p8_224_in21k',\n",
              " 'vit_tiny_r_s16_p8_384',\n",
              " 'vovnet39a',\n",
              " 'vovnet57a',\n",
              " 'wide_resnet50_2',\n",
              " 'wide_resnet101_2',\n",
              " 'xception',\n",
              " 'xception41',\n",
              " 'xception65',\n",
              " 'xception71']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx8cVLFOL7Yo"
      },
      "source": [
        "#loads model\n",
        "res_18 = timm.create_model(\n",
        "    model_name='resnet18',\n",
        "    pretrained=True,\n",
        "    num_classes=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBrYSMm8J-KT"
      },
      "source": [
        "###### SWIN\n",
        "\n",
        "https://github.com/microsoft/Swin-Transformer\n",
        "\n",
        "Swin Transformer (название Swin означает «Сдвинутое окно») изначально описан в [arxiv](https://arxiv.org/abs/2103.14030), который может служить универсальной основой для компьютерного зрения. \n",
        "\n",
        "По сути, это иерархический преобразователь, представление которого вычисляется со смещенными окнами. Схема смещения окон обеспечивает большую эффективность, ограничивая вычисление самовнимания неперекрывающимися локальными окнами, а также допускает межоконное соединение.\n",
        "\n",
        "Swin Transformer обеспечивает высокую производительность при обнаружении объектов COCO (58,7 прямоугольных AP и 51,1 маскированных AP на test-dev) и семантической сегментации ADE20K (53,5 mIoU на val), значительно превосходя предыдущие модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvXgE_kGP0a9"
      },
      "source": [
        "convnet.patch_embed.proj = nn.Conv2d(\n",
        "    bert.config.num_hidden_layers * bert.config.num_attention_heads,\n",
        "    96, kernel_size=(4, 4), stride=(4, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KHjDc9SN5M7"
      },
      "source": [
        "###### VIT\n",
        "https://huggingface.co/google/vit-base-patch16-224\n",
        "\n",
        "Vision Transformer (ViT) - это модель кодировщика трансформатора (подобная BERT), предварительно обученная на большой коллекции изображений контролируемым образом, а именно ImageNet-21k, с разрешением 224x224 пикселей. Затем модель была доработана в ImageNet (также называемом ILSVRC2012), наборе данных, включающем 1 миллион изображений и 1000 классов, также с разрешением 224x224.\n",
        "\n",
        "Изображения представлены модели в виде последовательности участков фиксированного размера (разрешение 16x16), которые встроены линейно. Также в начало последовательности добавляется токен [CLS], чтобы использовать его для задач классификации. Также добавляются вложения абсолютного положения перед подачей последовательности на уровни энкодера Transformer.\n",
        "\n",
        "Предварительно обучая модель, она изучает внутреннее представление изображений, которое затем можно использовать для извлечения функций, полезных для последующих задач: например, если у вас есть набор данных с помеченными изображениями, вы можете обучить стандартный классификатор, поместив линейный слой на верхняя часть предварительно обученного кодировщика. Обычно поверх токена [CLS] размещается линейный слой, поскольку последнее скрытое состояние этого токена можно рассматривать как представление всего изображения.\n",
        "\n",
        "Вы можете использовать необработанную модель для классификации изображений. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1H3f6hwN41f"
      },
      "source": [
        "convnet.patch_embed.proj = nn.Conv2d(\n",
        "    bert.config.num_hidden_layers * bert.config.num_attention_heads,\n",
        "    768, kernel_size=(32, 32), stride=(32, 32)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHZE65ZjQZKI"
      },
      "source": [
        "###### efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-if1DpMWN4zA"
      },
      "source": [
        "convnet.conv_stem = nn.Conv2d(\n",
        "    bert.config.num_hidden_layers * bert.config.num_attention_heads,\n",
        "    32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGUhfq7GQsR_"
      },
      "source": [
        "###### resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvBQLKL-N4tq"
      },
      "source": [
        "convnet.conv1 = nn.Conv2d(\n",
        "    bert.config.num_hidden_layers * bert.config.num_attention_heads,\n",
        "    64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wjJMoziQzuA"
      },
      "source": [
        "###### linear_conv_final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az4fd-y1m9Dv"
      },
      "source": [
        "linear_conv_final = nn.Sequential(\n",
        "    nn.Linear(convnet.num_features, cfg.linear_final_dim),\n",
        "    # nn.BatchNorm1d(cfg.linear_final_dim),\n",
        "    cfg.activation(),\n",
        "    nn.Dropout(cfg.dropout)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUo0_aIRuBuM"
      },
      "source": [
        "##### linear features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A_oFw8Hm9As"
      },
      "source": [
        "linear_feature = nn.Sequential(\n",
        "    nn.Linear(17, cfg.linear_final_dim),\n",
        "    cfg.activation(),\n",
        "    nn.Dropout(cfg.dropout)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lxk2Su7wJCc"
      },
      "source": [
        "##### lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppRTNLb3m88T"
      },
      "source": [
        " def make_lstm_module(self):\n",
        "    ret = []\n",
        "    if cfg.word_axis:\n",
        "        hidden_size = cfg.max_length * len(cfg.rnn_hidden_indice)\n",
        "    else:\n",
        "        hidden_size = bert.config.hidden_size * len(cfg.rnn_hidden_indice)\n",
        "\n",
        "    for i in range(cfg.rnn_module_num):\n",
        "        ret.append((f\"lstm_module_{i}\", LSTMModule(cfg=cfg, hidden_size=hidden_size)))\n",
        "        if cfg.bidirectional:\n",
        "            hidden_size = int(hidden_size * cfg.rnn_module_shrink_ratio * 2)\n",
        "        else:\n",
        "            hidden_size = int(hidden_size * cfg.rnn_module_shrink_ratio)\n",
        "    return nn.Sequential(OrderedDict(ret))\n",
        "\n",
        "\n",
        "lstm = make_lstm_module()\n",
        "if cfg.bidirectional:\n",
        "    if cfg.word_axis: # bool param\n",
        "        lstm_size = int(cfg.max_length * len(cfg.rnn_hidden_indice) * (\n",
        "            (2 * cfg.rnn_module_shrink_ratio) ** cfg.rnn_module_num))\n",
        "    else:\n",
        "        lstm_size = int(bert.config.hidden_size * len(cfg.rnn_hidden_indice) * (\n",
        "            (2 * cfg.rnn_module_shrink_ratio) ** cfg.rnn_module_num))\n",
        "else:\n",
        "    if cfg.word_axis:\n",
        "        lstm_size = int(cfg.max_length * len(cfg.rnn_hidden_indice) * (\n",
        "            cfg.rnn_module_shrink_ratio ** cfg.rnn_module_num))\n",
        "\n",
        "    else:\n",
        "        lstm_size = int(bert.config.hidden_size * len(cfg.rnn_hidden_indice) * (\n",
        "            cfg.rnn_module_shrink_ratio ** cfg.rnn_module_num))\n",
        "            \n",
        "            \n",
        "linear_lstm_final = nn.Sequential(\n",
        "    nn.Linear(lstm_size, cfg.linear_final_dim),\n",
        "    # nn.BatchNorm1d(cfg.linear_final_dim),\n",
        "    cfg.activation(),\n",
        "    nn.Dropout(cfg.dropout)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caA2eUZyzyjp"
      },
      "source": [
        "#### re-init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e22ZpxMvm833"
      },
      "source": [
        "def reinit_bert(self):\n",
        "    def get_model_type(x):\n",
        "        if \"distilbert\" in x: return \"distilbert\"\n",
        "        if \"albert\" in x: return \"albert\"\n",
        "        if \"roberta\" in x: return \"roberta\"\n",
        "        if \"bert\" in x: return \"bert\"\n",
        "\n",
        "    # re-init pooler\n",
        "    if cfg.reinit_pooler and not cfg.prep_enable:\n",
        "        if \"bert\" in cfg.nlp_model_name or \"roberta\" in cfg.nlp_model_name or \"luke\" in cfg.nlp_model_name:\n",
        "            bert.pooler.dense.weight.data.normal_(mean=0.0, std=bert.config.initializer_range)\n",
        "            bert.pooler.dense.bias.data.zero_()\n",
        "            for p in bert.pooler.parameters():\n",
        "                p.requires_grad = True\n",
        "        elif \"xlnet\" in cfg.nlp_model_name:\n",
        "            raise ValueError(f\"{cfg.nlp_model_name} does not have a pooler at the end\")\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    # re-init layers\n",
        "    if cfg.reinit_layers > 0:\n",
        "        if \"albert\" in cfg.nlp_model_name:\n",
        "            raise ValueError(\"albert not reinit\")\n",
        "\n",
        "        elif \"bert\" in cfg.nlp_model_name or \"roberta\" in cfg.nlp_model_name:\n",
        "            if cfg.prep_enable:\n",
        "                if get_model_type(cfg.nlp_model_name) == \"bert\":\n",
        "                    layers = bert.bert.encoder.layer[-cfg.reinit_layers:]\n",
        "                elif get_model_type(cfg.nlp_model_name) == \"roberta\":\n",
        "                    layers = bert.roberta.encoder.layer[-cfg.reinit_layers:]\n",
        "            else:\n",
        "                if get_model_type(cfg.nlp_model_name) == \"distilbert\":\n",
        "                    layers = bert.transformer.layer[-cfg.reinit_layers:]\n",
        "                else:\n",
        "                    layers = bert.encoder.layer[-cfg.reinit_layers:]\n",
        "            for layer in layers:\n",
        "                for module in layer.modules():\n",
        "                    if isinstance(module, nn.Linear):\n",
        "                        # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                        # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                        module.weight.data.normal_(mean=0.0, std=bert.config.initializer_range)\n",
        "                        if module.bias is not None:\n",
        "                            module.bias.data.zero_()\n",
        "                    elif isinstance(module, nn.Embedding):\n",
        "                        module.weight.data.normal_(mean=0.0, std=bert.config.initializer_range)\n",
        "                        if module.padding_idx is not None:\n",
        "                            module.weight.data[module.padding_idx].zero_()\n",
        "                    elif isinstance(module, nn.LayerNorm):\n",
        "                        module.bias.data.zero_()\n",
        "                        module.weight.data.fill_(1.0)\n",
        "\n",
        "        elif \"xlnet\" in cfg.nlp_model_name:\n",
        "            for layer in bert.layer[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "        elif \"luke\" in cfg.nlp_model_name:\n",
        "            if cfg.prep_enable:\n",
        "                raise NotImplementedError\n",
        "            else:\n",
        "                layers = bert.encoder.layer[-cfg.reinit_layers:]\n",
        "            for layer in layers:\n",
        "                for module in layer.modules():\n",
        "                    if isinstance(module, nn.Linear):\n",
        "                        # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                        # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                        module.weight.data.normal_(mean=0.0, std=bert.config.initializer_range)\n",
        "                        if module.bias is not None:\n",
        "                            module.bias.data.zero_()\n",
        "                    elif isinstance(module, nn.Embedding):\n",
        "                        module.weight.data.normal_(mean=0.0, std=bert.config.initializer_range)\n",
        "                        if module.padding_idx is not None:\n",
        "                            module.weight.data[module.padding_idx].zero_()\n",
        "                    elif isinstance(module, nn.LayerNorm):\n",
        "                        module.bias.data.zero_()\n",
        "                        module.weight.data.fill_(1.0)\n",
        "        elif \"funnel\" in cfg.nlp_model_name:\n",
        "            for layer in bert.encoder.blocks[2][-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "        elif \"bart\" in cfg.nlp_model_name:\n",
        "            for layer in bert.decoder.layers[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "            if not cfg.decoder_only:\n",
        "                for layer in bert.encoder.layers[-cfg.reinit_layers:]:\n",
        "                    for module in layer.modules():\n",
        "                        bert._init_weights(module)\n",
        "        elif \"electra\" in cfg.nlp_model_name:\n",
        "            for layer in bert.encoder.layer[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "        elif \"gpt2\" in cfg.nlp_model_name:\n",
        "            for layer in bert.h[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "        elif \"gpt-neo\" in cfg.nlp_model_name:\n",
        "            for layer in bert.h[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "        elif \"t5\" in cfg.nlp_model_name:\n",
        "            for layer in bert.encoder.block[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "            if not cfg.decoder_only:\n",
        "                for layer in bert.decoder.block[-cfg.reinit_layers:]:\n",
        "                    for module in layer.modules():\n",
        "                        bert._init_weights(module)\n",
        "        elif \"mpnet\" in cfg.nlp_model_name:\n",
        "            for layer in bert.encoder.layer[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "        elif \"layoutlm\" in cfg.nlp_model_name:\n",
        "            for layer in bert.encoder.layer[-cfg.reinit_layers:]:\n",
        "                for module in layer.modules():\n",
        "                    bert._init_weights(module)\n",
        "    \"\"\"\n",
        "    for layer in [linear1, linear2, linear1_std, linear2_std, linear_perp, linear_vocab,\n",
        "                    linear_tcn_final, linear_lstm_final, linear_hidden_final,\n",
        "                    linear_conv_final, linear_vocab_final]:\n",
        "        for module in layer.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.weight.data.normal_(mean=0.0, std=bert.config.initializer_range)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiKt7837HSg"
      },
      "source": [
        "#### forward layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txqmU2Om6PLg"
      },
      "source": [
        "def forward(self, input_ids_masked, attention_mask, token_type_ids, input_ids, features):\n",
        "    def f(x_in, perplexity=None):\n",
        "        x_in = F.dropout(x_in, p=cfg.multi_dropout_ratio, training=True)\n",
        "        if perplexity is not None:\n",
        "            x_out_mean = linear1(torch.cat([x_in, perplexity], dim=1))\n",
        "            x_out_mean = linear2(torch.cat([x_out_mean, perplexity], dim=1))\n",
        "        else:\n",
        "            x_out_mean = linear1(x_in)\n",
        "            x_out_mean = linear2(x_out_mean)\n",
        "        return x_out_mean\n",
        "\n",
        "    def g(x_in, perplexity=None):\n",
        "        x_in = F.dropout(x_in, p=cfg.multi_dropout_ratio, training=True)\n",
        "        if perplexity is not None:\n",
        "            x_out_std = linear1_std(torch.cat([x_in, perplexity], dim=1))\n",
        "            x_out_std = linear2_std(torch.cat([x_out_std, perplexity], dim=1))\n",
        "        else:\n",
        "            x_out_std = linear1(x_in)\n",
        "            x_out_std = linear2(x_out_std)\n",
        "        x_out_std = torch.exp(x_out_std) ** 0.5\n",
        "        return x_out_std\n",
        "\n",
        "    if not cfg.prep_enable:\n",
        "        x = bert(input_ids=input_ids_masked,\n",
        "                        attention_mask=attention_mask,\n",
        "                        output_attentions=True,\n",
        "                        output_hidden_states=True)\n",
        "        if \"deberta\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[1], x[2]]\n",
        "        elif \"xlnet\" in cfg.nlp_model_name:\n",
        "            if len(x) == 4:\n",
        "                x = [x[0], x[2], x[3], x[1]]\n",
        "            else:\n",
        "                x = [x[0], x[1], x[2]]\n",
        "        elif \"albert\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[1]]\n",
        "        elif \"distilbert\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[1], x[2]]\n",
        "        elif \"funnel\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[1], x[2]]\n",
        "        elif \"bart\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[2], x[3], None]  # x[2]: decoder hidden_states, x[3]: cross attention\n",
        "        elif \"electra\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[1], x[2]]\n",
        "        elif \"t5\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[1], x[2]]\n",
        "        elif \"mpnet\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[2], x[3], x[1]]\n",
        "        else:\n",
        "            x = [x[0], x[2], x[3], x[1]]\n",
        "    elif \"funnel\" in cfg.nlp_model_name:\n",
        "        x = bert.funnel(input_ids=input_ids_masked,\n",
        "                                attention_mask=attention_mask,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                output_attentions=True,\n",
        "                                output_hidden_states=True)\n",
        "        if cfg.prep_enable:\n",
        "            input_ids_pred = bert.lm_head(x[0])\n",
        "    elif \"albert\" in cfg.nlp_model_name:\n",
        "        x = bert.albert(input_ids=input_ids_masked,\n",
        "                                attention_mask=attention_mask,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                output_attentions=True,\n",
        "                                output_hidden_states=True)\n",
        "        if cfg.prep_enable:\n",
        "            input_ids_pred = bert.predictions(x[0])\n",
        "    elif \"deberta\" in cfg.nlp_model_name:\n",
        "        x = bert.deberta(input_ids=input_ids_masked,\n",
        "                                attention_mask=attention_mask,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                output_attentions=True,\n",
        "                                output_hidden_states=True)\n",
        "        if cfg.prep_enable:\n",
        "            input_ids_pred = bert.cls(x[0])\n",
        "    elif \"roberta\" in cfg.nlp_model_name and \"bigbird\" not in cfg.nlp_model_name:\n",
        "        x = bert.roberta(input_ids=input_ids_masked,\n",
        "                                attention_mask=attention_mask,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                output_attentions=True,\n",
        "                                output_hidden_states=True)\n",
        "        if cfg.prep_enable:\n",
        "            input_ids_pred = bert.lm_head(x[0])\n",
        "\n",
        "    elif \"bert\" in cfg.nlp_model_name or \"bigbird\" in cfg.nlp_model_name:\n",
        "        x = bert.bert(input_ids=input_ids_masked,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            output_attentions=True,\n",
        "                            output_hidden_states=True)\n",
        "        if cfg.prep_enable:\n",
        "            input_ids_pred = bert.cls(x[0])\n",
        "    else:\n",
        "        x = bert(input_ids=input_ids_masked,\n",
        "                        attention_mask=attention_mask,\n",
        "                        output_attentions=True,\n",
        "                        output_hidden_states=True)\n",
        "        if \"luke-base\" in cfg.nlp_model_name or \"luke-large\" in cfg.nlp_model_name:\n",
        "            x = [x[0], x[2], x[3]]\n",
        "\n",
        "    # x[0]: last hidden layer, x[1]: all hidden layer, x[2]: attention matrix\n",
        "    if cfg.prep_enable:\n",
        "        loss = torch.nn.functional.cross_entropy(input_ids_pred.view(-1, bert.config.vocab_size), input_ids.view(-1), reduction=\"none\")\n",
        "        perplexity = loss.view(len(input_ids), -1) * attention_mask\n",
        "        perplexity = perplexity.sum(dim=1) / attention_mask.sum(dim=1)\n",
        "        perplexity = perplexity.view(-1, 1)\n",
        "\n",
        "    # base feature\n",
        "    x_bert = []\n",
        "    if cfg.pooler_enable:\n",
        "        x_bert.append(x[3])\n",
        "    if cfg.linear_vocab_enable:\n",
        "        xx = dropout(linear_vocab(x[0]).view(len(input_ids), -1))\n",
        "        x_bert.append(linear_vocab_final(xx))\n",
        "    if cfg.self_attention_enable:\n",
        "        xx = torch.cat([dropout_attn(xx) for xx in x[2]], dim=1)\n",
        "        xx = convnet(xx)\n",
        "        xx = linear_conv_final(xx)\n",
        "        x_bert.append(xx)\n",
        "    if cfg.hidden_stack_enable:\n",
        "        if \"albert\" in cfg.nlp_model_name:\n",
        "            xx = linear_hidden_final(x[0].mean(dim=1))\n",
        "            x_bert.append(xx)\n",
        "        else:\n",
        "            if \"funnel\" in cfg.nlp_model_name:\n",
        "                xx = torch.stack([dropout_bert_stack(xx) for xx in x[1][-3:]]).mean(dim=[0, 2])\n",
        "            else:\n",
        "                xx = torch.stack([dropout_bert_stack(xx) for xx in x[1][-4:]]).mean(dim=0)\n",
        "                xx = torch.sum(\n",
        "                    xx * attention_mask.unsqueeze(-1), dim=1, keepdim=False\n",
        "                )\n",
        "                xx = xx / torch.sum(attention_mask, dim=-1, keepdim=True)\n",
        "            xx = linear_hidden_final(xx)\n",
        "            x_bert.append(xx)\n",
        "\n",
        "    # residual feature\n",
        "    if cfg.rnn_module_num > 0:\n",
        "        if cfg.word_axis:\n",
        "            x_lstm = lstm(torch.cat([x[1][idx] for idx in cfg.rnn_hidden_indice], dim=1).transpose(2, 1)).mean(dim=1)\n",
        "        else:\n",
        "            x_lstm = lstm(torch.cat([x[1][idx] for idx in cfg.rnn_hidden_indice], dim=2)).mean(dim=1)\n",
        "        x_lstm = linear_lstm_final(x_lstm)\n",
        "        x_bert.append(x_lstm)\n",
        "    if cfg.attention_head_enable:\n",
        "        weights_attn = attention_head(x[0])\n",
        "        x_attn = torch.sum(weights_attn * x[0], dim=1)\n",
        "        x_attn = linear_attention_head_final(x_attn)\n",
        "        x_bert.append(x_attn)\n",
        "    if cfg.tcn_module_enable:\n",
        "        if cfg.word_axis:\n",
        "            x_tcn = tcn(dropout(x[0])).mean(dim=1)\n",
        "        else:\n",
        "            x_tcn = tcn(dropout(x[0]).permute(0, 2, 1)).mean(dim=2)\n",
        "        x_tcn = linear_tcn_final(x_tcn)\n",
        "        x_bert.append(x_tcn)\n",
        "    if cfg.attention_pool_enable:\n",
        "        xx = torch.cat([xx for xx in x[2]], dim=1).mean(dim=1).reshape(len(input_ids), -1)\n",
        "        xx = linear_attention_pool_final(xx)\n",
        "        x_bert.append(xx)\n",
        "    if cfg.feature_enable:\n",
        "        xx = linear_feature(features)\n",
        "        x_bert.append(xx)\n",
        "\n",
        "    x_bert = torch.cat(x_bert, dim=1)\n",
        "\n",
        "    if cfg.prep_enable:\n",
        "        perplexity = linear_perp(perplexity)\n",
        "        x_out_mean = torch.stack([f(x_bert, perplexity) for _ in range(cfg.multi_dropout_num)]).mean(dim=0)\n",
        "        x_out_std = torch.stack([g(x_bert, perplexity) for _ in range(cfg.multi_dropout_num)]).mean(dim=0)\n",
        "    elif cfg.simple_structure:\n",
        "        x_out_mean = linear_simple(x_bert)\n",
        "        x_out_std = None\n",
        "    else:\n",
        "        x_out_mean = torch.stack([f(x_bert) for _ in range(cfg.multi_dropout_num)]).mean(dim=0)\n",
        "        x_out_std = torch.stack([g(x_bert) for _ in range(cfg.multi_dropout_num)]).mean(dim=0)\n",
        "        return x_out_mean, x_out_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h7j_Zai6JC6"
      },
      "source": [
        "#### optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBdQTZSAm807"
      },
      "source": [
        "def configure_optimizers(self):\n",
        "    def extract_params(named_parameters, lr, weight_decay, no_decay=False):\n",
        "        ret = {}\n",
        "        no_decay_ary = [\"bias\", \"LayerNorm.weight\", \"pooler\"]\n",
        "\n",
        "        if no_decay:\n",
        "            ret[\"params\"] = [p for n, p in named_parameters if not any(nd in n for nd in no_decay_ary)]\n",
        "            ret[\"weight_decay\"] = 0\n",
        "        else:\n",
        "            ret[\"params\"] = [p for n, p in named_parameters if any(nd in n for nd in no_decay_ary) and \"pooler\" not in n]\n",
        "            ret[\"weight_decay\"] = weight_decay\n",
        "        ret[\"lr\"] = lr\n",
        "        return ret\n",
        "\n",
        "    params = []\n",
        "    if cfg.prep_enable:\n",
        "        if \"funnel\" in cfg.nlp_model_name:\n",
        "            params.append({\"params\": bert.lm_head.parameters(), \"weight_decay\": cfg.weight_decay, \"lr\": cfg.lr_bert})\n",
        "        elif \"albert\" in cfg.nlp_model_name:\n",
        "            params.append({\"params\": bert.predictions.parameters(), \"weight_decay\": cfg.weight_decay, \"lr\": cfg.lr_bert})\n",
        "        elif \"deberta\" in cfg.nlp_model_name:\n",
        "            params.append({\"params\": bert.cls.parameters(), \"weight_decay\": cfg.weight_decay, \"lr\": cfg.lr_bert})\n",
        "        elif \"roberta\" in cfg.nlp_model_name and \"bigbird\" not in cfg.nlp_model_name:\n",
        "            params.append({\"params\": bert.lm_head.parameters(), \"weight_decay\": cfg.weight_decay, \"lr\": cfg.lr_bert})\n",
        "        elif \"bert\" in cfg.nlp_model_name or \"bigbird\" in cfg.nlp_model_name:\n",
        "            params.append({\"params\": bert.cls.parameters(), \"weight_decay\": cfg.weight_decay, \"lr\": cfg.lr_bert})\n",
        "        else:\n",
        "            raise ValueError(\"mask用のparameterありません\")\n",
        "    params.append(extract_params(bert.named_parameters(), lr=cfg.lr_bert, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "    params.append(extract_params(bert.named_parameters(), lr=cfg.lr_bert, weight_decay=0, no_decay=True))\n",
        "\n",
        "    if cfg.linear_vocab_enable:\n",
        "        params.append(extract_params(linear_vocab.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_vocab.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(linear_vocab_final.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_vocab_final.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "    if cfg.self_attention_enable:\n",
        "        params.append(extract_params(linear_conv_final.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_conv_final.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(convnet.named_parameters(), lr=cfg.lr_cnn, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(convnet.named_parameters(), lr=cfg.lr_cnn, weight_decay=0, no_decay=True))\n",
        "    if cfg.attention_pool_enable:\n",
        "        params.append(extract_params(linear_attention_pool_final.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_attention_pool_final.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "    if cfg.tcn_module_enable:\n",
        "        params.append(extract_params(linear_tcn_final.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_tcn_final.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(tcn.named_parameters(), lr=cfg.lr_tcn, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(tcn.named_parameters(), lr=cfg.lr_tcn, weight_decay=0, no_decay=True))\n",
        "    if cfg.rnn_module_num > 0:\n",
        "        params.append(extract_params(linear_lstm_final.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_lstm_final.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(lstm.named_parameters(), lr=cfg.lr_rnn, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(lstm.named_parameters(), lr=cfg.lr_rnn, weight_decay=0, no_decay=True))\n",
        "    if cfg.hidden_stack_enable:\n",
        "        params.append(extract_params(linear_hidden_final.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_hidden_final.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "    if cfg.simple_structure:\n",
        "        params.append(extract_params(linear_simple.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_simple.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "    else:\n",
        "        params.append(extract_params(linear1.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear1.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(linear2.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear2.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(linear1_std.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear1_std.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(linear2_std.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear2_std.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "    if cfg.attention_head_enable:\n",
        "        params.append(extract_params(attention_head.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(attention_head.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "        params.append(extract_params(linear_attention_head_final.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_attention_head_final.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "    if cfg.pooler_enable:\n",
        "        params.append(extract_params(bert.pooler.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(bert.pooler.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "    if cfg.feature_enable:\n",
        "        params.append(extract_params(linear_feature.named_parameters(), lr=cfg.lr_fc, weight_decay=cfg.weight_decay, no_decay=False))\n",
        "        params.append(extract_params(linear_feature.named_parameters(), lr=cfg.lr_fc, weight_decay=0, no_decay=True))\n",
        "\n",
        "    optimizer = cfg.optimizer(params)\n",
        "    num_warmup_steps = int(cfg.epochs_max * len(df_train) / cfg.batch_size * cfg.warmup_ratio)\n",
        "    num_training_steps = int(cfg.epochs_max * len(df_train) / cfg.batch_size) * cfg.training_steps_ratio\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=num_warmup_steps,\n",
        "                                                num_training_steps=num_training_steps)\n",
        "    return [optimizer], [scheduler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6CDUDwyzRvT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m57ICXOJzRsh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIFGsd4nzRo-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0I1b-N3z5WO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2UCQ8Ncz5TW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEzmYrsxz5Qd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPXWgJg2z5No"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZqTbCtTz5Kx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l27o3LORz5Hm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwnIcX5Jz4_T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUTi3E9hRQro"
      },
      "source": [
        "https://seasongid.com/season-3454-boston_legal.html\n",
        "10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLTPhfBmRQo3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}