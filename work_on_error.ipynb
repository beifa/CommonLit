{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "work_on_error.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Mkusuxvxk7LrYXbpX_GBImtIi0n2BqCC",
      "authorship_tag": "ABX9TyM/PTUi9+TeVCIyhWw8sTR8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqTeSQ9XezJ"
      },
      "source": [
        "## Description\n",
        "\n",
        "I wants understand why my train is different on other people where i wrong?\n",
        "I train default how i train another model detection and convolution but i see in kernels in kaggle people use another techicks to train nlp model and i can't understand why and i want test it.\n",
        "\n",
        "Links to use:\n",
        "- [22nd solution](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257302)\n",
        "\n",
        "- [gtihub link](https://github.com/kurupical/commonlit)\n",
        "\n",
        "- [RoBERTa Base Fine-Tuning with Better Training Strategies](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efA57ChrZV1l"
      },
      "source": [
        "## default or not changed loads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KsrTcKtZTwC"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install transformers\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJZ_KqlNZcqQ"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import  DataLoader, Dataset\n",
        "import transformers\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "path_tr = '/content/drive/MyDrive/CommonLit/input/train.csv'\n",
        "path_test = '/content/drive/MyDrive/CommonLit/input/test.csv'\n",
        "path_sub = '/content/drive/MyDrive/CommonLit/input/sample_submission.csv'\n",
        "\n",
        "\n",
        "SEED =13\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def make_folds(data: pd.DataFrame, split: int = 5):\n",
        "    data['kfold'] = -1\n",
        "    data = data.sample(frac =1).reset_index(drop=True)\n",
        "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
        "    data.loc[:, 'bins'] = pd.cut(\n",
        "        data['target'], bins = num_bins, labels = False\n",
        "    )\n",
        "    kf = model_selection.StratifiedKFold(n_splits=split)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z]+', ' ',str(txt).lower())\n",
        "\n",
        "\n",
        "df = pd.read_csv(path_tr)\n",
        "df['txt'] = df['excerpt']#.apply(lambda x: clean_text(x))\n",
        "df_folds = make_folds(df, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2h9W_fyZhRG"
      },
      "source": [
        "class CL_Dataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        token,\n",
        "        max_len: int = 256,\n",
        "        test: bool = False\n",
        "        ) -> dict:\n",
        "        self.data = data \n",
        "        self.max_len = max_len\n",
        "        self.test = test\n",
        "        self.token = token\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def  __getitem__(self, idx: int):\n",
        "        text = self.data.txt.iloc[idx]\n",
        "        encode = self.token(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True,            \n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False  \n",
        "            )\n",
        "        if self.test:\n",
        "            target = 0\n",
        "        else:\n",
        "            target = self.data.target.iloc[idx]\n",
        "\n",
        "        ids = encode[\"input_ids\"]\n",
        "        mask = encode[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            'target': torch.tensor(target, dtype = torch.float)  \n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFTGY6cBagJZ"
      },
      "source": [
        "def loss_fn(output,target):\n",
        "    return torch.sqrt(nn.MSELoss()(output,target))\n",
        "\n",
        "\n",
        "def viz_curve(data: dict, title: str) -> None:\n",
        "    epochs = list(range(1, EPOCH + 1))\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        specs=[\n",
        "               [{\"colspan\": 2}, None],\n",
        "               [{}, {}],\n",
        "               ],\n",
        "               vertical_spacing=0.09,\n",
        "               subplot_titles=('Loss Curve',  'RMSE', 'LR')\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['train_loss'],\n",
        "            mode='lines+markers',\n",
        "            name='Train Loss'),\n",
        "            row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['valid_loss'],\n",
        "            mode='lines+markers',\n",
        "            name='Valid Loss'),\n",
        "            row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['valid_rms'],\n",
        "            mode='lines+markers',\n",
        "            name='RMSE'),     \n",
        "            row=2, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['lr'],\n",
        "            mode='lines+markers',\n",
        "            name='LR'), \n",
        "            row=2, col=2\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        height=600,\n",
        "        width=600,\n",
        "        showlegend=False,\n",
        "        title = title,\n",
        "        margin=dict(l=10, r=10, t=30, b=20),                     \n",
        "        template=\"plotly_dark\"    \n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def update_result(\n",
        "    data: dict,\n",
        "    predict: torch.tensor,\n",
        "    target:torch.tensor\n",
        ") -> None:\n",
        "    loss = loss_fn(predict.squeeze(-1), target)\n",
        "    data['losses'].append(loss.detach().cpu().numpy())\n",
        "    data['all_pred'].append(predict.squeeze(-1).detach().cpu().numpy())\n",
        "    data['all_target'].append(target.detach().cpu().numpy())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train(\n",
        "    model:nn.Module, \n",
        "    loader: DataLoader,\n",
        "    optimizer: transformers.AdamW,\n",
        "    schedule: transformers.get_linear_schedule_with_warmup,\n",
        "    batch: int,\n",
        "    max_lenght: int\n",
        ") -> list:\n",
        "    model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    results = defaultdict(list)\n",
        "    for input in loader:\n",
        "        optimizer.zero_grad()\n",
        "        target = input['target'].to(device)\n",
        "        batch = {k:v.to(device) for k,v in input.items() if k != 'target'}   \n",
        "        out = model(**batch)\n",
        "        out = out.logits # BertForSequenceClassification\n",
        "        loss = update_result(results, out, target)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "        optimizer.step()    \n",
        "        schedule.step()\n",
        "    losses = np.mean(results['losses'])\n",
        "    allt = np.concatenate(results['all_target'])\n",
        "    allp = np.concatenate(results['all_pred'])\n",
        "    rmse = np.sqrt(mean_squared_error(allt, allp))  \n",
        "    return losses, rmse\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model:nn.Module, \n",
        "    loader: DataLoader, \n",
        "    batch: int,\n",
        "    max_lenght: int\n",
        ") -> list:\n",
        "    model.eval()\n",
        "    results = defaultdict(list)\n",
        "    for input in loader:  \n",
        "        target = input['target'].to(device)\n",
        "        batch = {k:v.to(device) for k,v in input.items() if k != 'target'} \n",
        "        out = model(**batch)\n",
        "        out = out.logits # BertForSequenceClassification\n",
        "        _ = update_result(results, out, target)\n",
        "    losses = np.mean(results['losses'])\n",
        "    allt = np.concatenate(results['all_target'])\n",
        "    allp = np.concatenate(results['all_pred'])\n",
        "    rmse = np.sqrt(mean_squared_error(allt, allp)) \n",
        "    return losses, rmse\n",
        "\n",
        "\n",
        "def showtime(model:nn.Module, data: pd.DataFrame, tokenizer:transformers.AutoTokenizer, fold: int) -> dict:    \n",
        "    history = defaultdict(list)\n",
        "    model = model.to(device)\n",
        "    ttr = data[data.kfold != fold].reset_index(drop=True)\n",
        "    vvl = data[data.kfold == fold].reset_index(drop=True)\n",
        "    print(f'Fold: {fold + 1}, --- {ttr.shape, vvl.shape}')\n",
        "\n",
        "    tr = CL_Dataset(ttr,tokenizer, MAX_LEN)\n",
        "    vl = CL_Dataset(vvl,tokenizer, MAX_LEN)\n",
        "    tr_loader = DataLoader(\n",
        "        tr,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=True\n",
        "    )\n",
        "    vl_loader = DataLoader(\n",
        "        vl,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=False\n",
        "    )\n",
        "    optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=True)\n",
        "    steps = len(tr_loader) * EPOCH\n",
        "    print(steps, len(tr_loader))\n",
        "    # steps = len(tr_loader)/BATCH * EPOCH\n",
        "    lin_schedule = transformers.get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=1,\n",
        "        num_training_steps=steps\n",
        "    )\n",
        "    best_rmse = np.inf\n",
        "    for epoch in tqdm(range(EPOCH)):\n",
        "        tr_loss, tr_rmse = train(model, tr_loader, optimizer, lin_schedule, BATCH, MAX_LEN)\n",
        "        vl_loss, vl_rmse = validate(model, vl_loader, BATCH, MAX_LEN)\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['valid_loss'].append(vl_loss)\n",
        "        history['valid_rms'].append(vl_rmse)\n",
        "        history['lr'].append(lr)\n",
        "        print(f'Epoch: {epoch}, lr: {lr}, train rmse: {tr_rmse}, vl rmse: {vl_rmse}, vl loss: {vl_loss}')\n",
        "        if vl_rmse < best_rmse:\n",
        "            print(f'Save rmse: {vl_rmse}')\n",
        "            torch.save(model.state_dict(), f'{MODEL}_model_{fold}.pth')\n",
        "            best_rmse = vl_rmse\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7sEEGipHsfF"
      },
      "source": [
        "class Auto_NLP_model:\n",
        "    def __init__(self, name: str, n_labels: int = 1):\n",
        "        print(f'Auto NLP init model : {name}')\n",
        "        self.name = name\n",
        "        self.n_labels=  n_labels        \n",
        "    \n",
        "    def loads(self):\n",
        "        print('Loading configuraiton...')\n",
        "        model_config = transformers.AutoConfig.from_pretrained(\n",
        "                pretrained_model_name_or_path=self.name,\n",
        "                num_labels=self.n_labels)\n",
        "        print('Loading tokenizer...')\n",
        "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "                pretrained_model_name_or_path=self.name)\n",
        "        print('Loading model...')\n",
        "        model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name_or_path=self.name,\n",
        "                config=model_config)\n",
        "        return model, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnrkOLV73K4h"
      },
      "source": [
        "\"\"\"\n",
        "https://huggingface.co/transformers/pretrained_models.html\n",
        "https://huggingface.co/transformers/model_doc/auto.html\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "m = {\n",
        "    # 'xlm-roberta':'xlm-roberta-base',\n",
        "    # 'deberta': 'microsoft/deberta-base',\n",
        "    # 'xlm':'xlm-mlm-ende-1024',\n",
        "    # 'camembert': 'camembert-base',\n",
        "    # 'mbart': 'facebook/mbart-large-cc25', #memory batch 4 not ends error\n",
        "    'roberta': 'roberta-base' #roberta-large'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qilBpxo_9ude",
        "outputId": "db7af14a-dced-4df4-a787-7b9fed27b5e8"
      },
      "source": [
        "EPOCH = 15\n",
        "BATCH = 6\n",
        "MAX_LEN = 256\n",
        "\n",
        "for k, v in m.items():\n",
        "    print(k)\n",
        "    model, tokenizer = Auto_NLP_model(v).loads()\n",
        "    MODEL = model.__class__.__name__\n",
        "    fold = 0\n",
        "    title = f'Model: {k} Fold: {fold + 1}'\n",
        "    history = showtime(model, df_folds, tokenizer, fold)\n",
        "    viz_curve(history, title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta\n",
            "Auto NLP init model : roberta-base\n",
            "Loading configuraiton...\n",
            "Loading tokenizer...\n",
            "Loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 1, --- ((2267, 8), (567, 8))\n",
            "5670 378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, lr: 1.866995942847063e-05, train rmse: 0.7436621189117432, vl rmse: 0.6540188789367676, vl loss: 0.6269398927688599\n",
            "Save rmse: 0.6540188789367676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [04:03<56:50, 243.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, lr: 1.7336390897865588e-05, train rmse: 0.5260870456695557, vl rmse: 0.6349746584892273, vl loss: 0.6099616289138794\n",
            "Save rmse: 0.6349746584892273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 3/15 [12:09<48:35, 242.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2, lr: 1.6002822367260542e-05, train rmse: 0.4299793243408203, vl rmse: 0.6682462096214294, vl loss: 0.6423256397247314\n",
            "Epoch: 3, lr: 1.4669253836655498e-05, train rmse: 0.36037909984588623, vl rmse: 0.543418824672699, vl loss: 0.5201668739318848\n",
            "Save rmse: 0.543418824672699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 5/15 [20:15<40:29, 242.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4, lr: 1.333568530605045e-05, train rmse: 0.3064141869544983, vl rmse: 0.5811491012573242, vl loss: 0.5575568079948425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [24:17<36:24, 242.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5, lr: 1.2002116775445406e-05, train rmse: 0.2750021517276764, vl rmse: 0.582738995552063, vl loss: 0.5591065883636475\n",
            "Epoch: 6, lr: 1.066854824484036e-05, train rmse: 0.241420716047287, vl rmse: 0.5221829414367676, vl loss: 0.4948228895664215\n",
            "Save rmse: 0.5221829414367676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 8/15 [32:23<28:20, 242.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7, lr: 9.334979714235315e-06, train rmse: 0.22280079126358032, vl rmse: 0.5436638593673706, vl loss: 0.5197162628173828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [36:26<24:16, 242.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8, lr: 8.001411183630271e-06, train rmse: 0.1977776437997818, vl rmse: 0.5721029043197632, vl loss: 0.5503267645835876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [40:30<20:16, 243.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 9, lr: 6.667842653025225e-06, train rmse: 0.18672426044940948, vl rmse: 0.5271970629692078, vl loss: 0.5030388236045837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [44:35<16:14, 243.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, lr: 5.33427412242018e-06, train rmse: 0.16900299489498138, vl rmse: 0.5574540495872498, vl loss: 0.5334554314613342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [48:39<12:11, 243.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11, lr: 4.0007055918151355e-06, train rmse: 0.15365661680698395, vl rmse: 0.5263659954071045, vl loss: 0.5023990273475647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [52:44<08:08, 244.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12, lr: 2.66713706121009e-06, train rmse: 0.14275813102722168, vl rmse: 0.585809588432312, vl loss: 0.562573254108429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [56:48<04:04, 244.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13, lr: 1.333568530605045e-06, train rmse: 0.13657480478286743, vl rmse: 0.5636139512062073, vl loss: 0.5399689674377441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [1:00:53<00:00, 243.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 14, lr: 0.0, train rmse: 0.12706246972084045, vl rmse: 0.5606884360313416, vl loss: 0.5367791652679443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"e6e78165-9f11-49c4-9242-666f661c33f0\" class=\"plotly-graph-div\" style=\"height:600px; width:600px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"e6e78165-9f11-49c4-9242-666f661c33f0\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'e6e78165-9f11-49c4-9242-666f661c33f0',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"Train Loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"xaxis\": \"x\", \"y\": [0.7005594372749329, 0.5061014294624329, 0.41116461157798767, 0.3445952832698822, 0.2917669415473938, 0.2610720992088318, 0.22887785732746124, 0.21034856140613556, 0.18829938769340515, 0.17742207646369934, 0.16159529983997345, 0.1463158279657364, 0.13561250269412994, 0.13032947480678558, 0.1211540475487709], \"yaxis\": \"y\"}, {\"mode\": \"lines+markers\", \"name\": \"Valid Loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"xaxis\": \"x\", \"y\": [0.6269398927688599, 0.6099616289138794, 0.6423256397247314, 0.5201668739318848, 0.5575568079948425, 0.5591065883636475, 0.4948228895664215, 0.5197162628173828, 0.5503267645835876, 0.5030388236045837, 0.5334554314613342, 0.5023990273475647, 0.562573254108429, 0.5399689674377441, 0.5367791652679443], \"yaxis\": \"y\"}, {\"mode\": \"lines+markers\", \"name\": \"RMSE\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"xaxis\": \"x2\", \"y\": [0.6540188789367676, 0.6349746584892273, 0.6682462096214294, 0.543418824672699, 0.5811491012573242, 0.582738995552063, 0.5221829414367676, 0.5436638593673706, 0.5721029043197632, 0.5271970629692078, 0.5574540495872498, 0.5263659954071045, 0.585809588432312, 0.5636139512062073, 0.5606884360313416], \"yaxis\": \"y2\"}, {\"mode\": \"lines+markers\", \"name\": \"LR\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"xaxis\": \"x3\", \"y\": [1.866995942847063e-05, 1.7336390897865588e-05, 1.6002822367260542e-05, 1.4669253836655498e-05, 1.333568530605045e-05, 1.2002116775445406e-05, 1.066854824484036e-05, 9.334979714235315e-06, 8.001411183630271e-06, 6.667842653025225e-06, 5.33427412242018e-06, 4.0007055918151355e-06, 2.66713706121009e-06, 1.333568530605045e-06, 0.0], \"yaxis\": \"y3\"}],\n",
              "                        {\"annotations\": [{\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Loss Curve\", \"x\": 0.5, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"RMSE\", \"x\": 0.225, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 0.455, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"LR\", \"x\": 0.775, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 0.455, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"height\": 600, \"margin\": {\"b\": 20, \"l\": 10, \"r\": 10, \"t\": 30}, \"showlegend\": false, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#f2f5fa\"}, \"error_y\": {\"color\": \"#f2f5fa\"}, \"marker\": {\"line\": {\"color\": \"rgb(17,17,17)\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"rgb(17,17,17)\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#A2B1C6\", \"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"minorgridcolor\": \"#506784\", \"startlinecolor\": \"#A2B1C6\"}, \"baxis\": {\"endlinecolor\": \"#A2B1C6\", \"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"minorgridcolor\": \"#506784\", \"startlinecolor\": \"#A2B1C6\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"line\": {\"color\": \"#283442\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"line\": {\"color\": \"#283442\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#506784\"}, \"line\": {\"color\": \"rgb(17,17,17)\"}}, \"header\": {\"fill\": {\"color\": \"#2a3f5f\"}, \"line\": {\"color\": \"rgb(17,17,17)\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#f2f5fa\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#f2f5fa\"}, \"geo\": {\"bgcolor\": \"rgb(17,17,17)\", \"lakecolor\": \"rgb(17,17,17)\", \"landcolor\": \"rgb(17,17,17)\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"#506784\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"dark\"}, \"paper_bgcolor\": \"rgb(17,17,17)\", \"plot_bgcolor\": \"rgb(17,17,17)\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}, \"bgcolor\": \"rgb(17,17,17)\", \"radialaxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"rgb(17,17,17)\", \"gridcolor\": \"#506784\", \"gridwidth\": 2, \"linecolor\": \"#506784\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"#C8D4E3\"}, \"yaxis\": {\"backgroundcolor\": \"rgb(17,17,17)\", \"gridcolor\": \"#506784\", \"gridwidth\": 2, \"linecolor\": \"#506784\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"#C8D4E3\"}, \"zaxis\": {\"backgroundcolor\": \"rgb(17,17,17)\", \"gridcolor\": \"#506784\", \"gridwidth\": 2, \"linecolor\": \"#506784\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"#C8D4E3\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#f2f5fa\"}}, \"sliderdefaults\": {\"bgcolor\": \"#C8D4E3\", \"bordercolor\": \"rgb(17,17,17)\", \"borderwidth\": 1, \"tickwidth\": 0}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}, \"bgcolor\": \"rgb(17,17,17)\", \"caxis\": {\"gridcolor\": \"#506784\", \"linecolor\": \"#506784\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"updatemenudefaults\": {\"bgcolor\": \"#506784\", \"borderwidth\": 0}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"#283442\", \"linecolor\": \"#506784\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"#283442\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"#283442\", \"linecolor\": \"#506784\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"#283442\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Model: roberta Fold: 1\"}, \"width\": 600, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0]}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.0, 0.45]}, \"xaxis3\": {\"anchor\": \"y3\", \"domain\": [0.55, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.545, 1.0]}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 0.455]}, \"yaxis3\": {\"anchor\": \"x3\", \"domain\": [0.0, 0.455]}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e6e78165-9f11-49c4-9242-666f661c33f0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wslT6E7nbtuG"
      },
      "source": [
        "### results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pgWHou-08Sq"
      },
      "source": [
        "results model roberta-large(~1h:45min):\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "Fold: 1, --- ((2267, 8), (567, 8))\n",
        "  0%|          | 0/15 [00:00<?, ?it/s]Epoch: 0, lr: 1.866995942847063e-05, train rmse: 0.771242618560791, vl rmse: 0.5320066809654236, vl loss: 0.5065193772315979\n",
        "Save rmse: 0.5320066809654236\n",
        " 13%|█▎        | 2/15 [13:49<1:30:08, 416.03s/it]Epoch: 1, lr: 1.7336390897865588e-05, train rmse: 0.5480484366416931, vl rmse: 0.7261704802513123, vl loss: 0.7056165933609009\n",
        " 20%|██        | 3/15 [20:51<1:23:43, 418.63s/it]Epoch: 2, lr: 1.6002822367260542e-05, train rmse: 0.45481783151626587, vl rmse: 0.5933656096458435, vl loss: 0.5674742460250854\n",
        " 27%|██▋       | 4/15 [27:52<1:16:56, 419.67s/it]Epoch: 3, lr: 1.4669253836655498e-05, train rmse: 0.3679790794849396, vl rmse: 0.6310817003250122, vl loss: 0.6084640622138977\n",
        " 33%|███▎      | 5/15 [34:54<1:10:04, 420.41s/it]Epoch: 4, lr: 1.333568530605045e-05, train rmse: 0.3390931189060211, vl rmse: 0.7398288249969482, vl loss: 0.721051037311554\n",
        " 40%|████      | 6/15 [41:56<1:03:08, 420.91s/it]Epoch: 5, lr: 1.2002116775445406e-05, train rmse: 0.2956746220588684, vl rmse: 0.5392723083496094, vl loss: 0.5154401659965515\n",
        " 47%|████▋     | 7/15 [48:58<56:11, 421.40s/it]  Epoch: 6, lr: 1.066854824484036e-05, train rmse: 0.2643064260482788, vl rmse: 0.5453745126724243, vl loss: 0.5227868556976318\n",
        " 53%|█████▎    | 8/15 [56:01<49:11, 421.65s/it]Epoch: 7, lr: 9.334979714235315e-06, train rmse: 0.25952309370040894, vl rmse: 0.6315166354179382, vl loss: 0.6088392734527588\n",
        " 60%|██████    | 9/15 [1:03:03<42:11, 421.85s/it]Epoch: 8, lr: 8.001411183630271e-06, train rmse: 0.2210676074028015, vl rmse: 0.659906804561615, vl loss: 0.6411230564117432\n",
        " 67%|██████▋   | 10/15 [1:10:05<35:10, 422.07s/it]Epoch: 9, lr: 6.667842653025225e-06, train rmse: 0.21520286798477173, vl rmse: 0.5818750262260437, vl loss: 0.5584777593612671\n",
        "Epoch: 10, lr: 5.33427412242018e-06, train rmse: 0.20328344404697418, vl rmse: 0.5239043235778809, vl loss: 0.4989898204803467\n",
        "Save rmse: 0.5239043235778809\n",
        " 80%|████████  | 12/15 [1:24:15<21:09, 423.21s/it]Epoch: 11, lr: 4.0007055918151355e-06, train rmse: 0.18055203557014465, vl rmse: 0.5367754697799683, vl loss: 0.5130714178085327\n",
        " 87%|████████▋ | 13/15 [1:31:17<14:05, 422.95s/it]Epoch: 12, lr: 2.66713706121009e-06, train rmse: 0.1665629744529724, vl rmse: 0.5970629453659058, vl loss: 0.5752689838409424\n",
        " 93%|█████████▎| 14/15 [1:38:19<07:02, 422.65s/it]Epoch: 13, lr: 1.333568530605045e-06, train rmse: 0.15745727717876434, vl rmse: 0.5785849094390869, vl loss: 0.5563144683837891\n",
        "100%|██████████| 15/15 [1:45:21<00:00, 421.43s/it]Epoch: 14, lr: 0.0, train rmse: 0.14649353921413422, vl rmse: 0.5729362964630127, vl loss: 0.5512558817863464\n",
        "\n",
        "make small version robrta base 1h:\n",
        "\n",
        "Fold: 1, --- ((2267, 8), (567, 8))\n",
        "5670 378\n",
        "  0%|          | 0/15 [00:00<?, ?it/s]Epoch: 0, lr: 1.866995942847063e-05, train rmse: 0.7436621189117432, vl rmse: 0.6540188789367676, vl loss: 0.6269398927688599\n",
        "Save rmse: 0.6540188789367676\n",
        "  7%|▋         | 1/15 [04:03<56:50, 243.59s/it]Epoch: 1, lr: 1.7336390897865588e-05, train rmse: 0.5260870456695557, vl rmse: 0.6349746584892273, vl loss: 0.6099616289138794\n",
        "Save rmse: 0.6349746584892273\n",
        " 20%|██        | 3/15 [12:09<48:35, 242.93s/it]Epoch: 2, lr: 1.6002822367260542e-05, train rmse: 0.4299793243408203, vl rmse: 0.6682462096214294, vl loss: 0.6423256397247314\n",
        "Epoch: 3, lr: 1.4669253836655498e-05, train rmse: 0.36037909984588623, vl rmse: 0.543418824672699, vl loss: 0.5201668739318848\n",
        "Save rmse: 0.543418824672699\n",
        " 33%|███▎      | 5/15 [20:15<40:29, 242.91s/it]Epoch: 4, lr: 1.333568530605045e-05, train rmse: 0.3064141869544983, vl rmse: 0.5811491012573242, vl loss: 0.5575568079948425\n",
        " 40%|████      | 6/15 [24:17<36:24, 242.74s/it]Epoch: 5, lr: 1.2002116775445406e-05, train rmse: 0.2750021517276764, vl rmse: 0.582738995552063, vl loss: 0.5591065883636475\n",
        "Epoch: 6, lr: 1.066854824484036e-05, train rmse: 0.241420716047287, vl rmse: 0.5221829414367676, vl loss: 0.4948228895664215\n",
        "Save rmse: 0.5221829414367676\n",
        " 53%|█████▎    | 8/15 [32:23<28:20, 242.88s/it]Epoch: 7, lr: 9.334979714235315e-06, train rmse: 0.22280079126358032, vl rmse: 0.5436638593673706, vl loss: 0.5197162628173828\n",
        " 60%|██████    | 9/15 [36:26<24:16, 242.72s/it]Epoch: 8, lr: 8.001411183630271e-06, train rmse: 0.1977776437997818, vl rmse: 0.5721029043197632, vl loss: 0.5503267645835876\n",
        " 67%|██████▋   | 10/15 [40:30<20:16, 243.21s/it]Epoch: 9, lr: 6.667842653025225e-06, train rmse: 0.18672426044940948, vl rmse: 0.5271970629692078, vl loss: 0.5030388236045837\n",
        " 73%|███████▎  | 11/15 [44:35<16:14, 243.60s/it]Epoch: 10, lr: 5.33427412242018e-06, train rmse: 0.16900299489498138, vl rmse: 0.5574540495872498, vl loss: 0.5334554314613342\n",
        " 80%|████████  | 12/15 [48:39<12:11, 243.90s/it]Epoch: 11, lr: 4.0007055918151355e-06, train rmse: 0.15365661680698395, vl rmse: 0.5263659954071045, vl loss: 0.5023990273475647\n",
        " 87%|████████▋ | 13/15 [52:44<08:08, 244.14s/it]Epoch: 12, lr: 2.66713706121009e-06, train rmse: 0.14275813102722168, vl rmse: 0.585809588432312, vl loss: 0.562573254108429\n",
        " 93%|█████████▎| 14/15 [56:48<04:04, 244.24s/it]Epoch: 13, lr: 1.333568530605045e-06, train rmse: 0.13657480478286743, vl rmse: 0.5636139512062073, vl loss: 0.5399689674377441\n",
        "100%|██████████| 15/15 [1:00:53<00:00, 243.57s/it]Epoch: 14, lr: 0.0, train rmse: 0.12706246972084045, vl rmse: 0.5606884360313416, vl loss: 0.5367791652679443\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8pEUOfA1YgU"
      },
      "source": [
        "## adds something new\n",
        "readme:\n",
        "- https://huggingface.co/transformers/model_summary.html\n",
        " \n",
        "\n",
        "TODO:\n",
        "- https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt\n",
        "- https://huggingface.co/blog/accelerate-library\n",
        "\n",
        "PS:\n",
        "- https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257629\n",
        "\n",
        "https://huggingface.co/course/chapter1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtNNZ-vUNK9v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl7itTSpNSxo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj4b4iIrNSu8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPFvxU9TNSpl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFCDM1dtNSmw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfSi8Jp-NShD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Y470BBfDbj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtSGAn-GfDZV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aso81UbfDXI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwH_L2KxfDVC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CsXM1nZfDTC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "yug3et9BfDQm",
        "outputId": "73f85a2b-2bfa-47b8-f219-95482ce2cfed"
      },
      "source": [
        "stop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXruqWJa9-_7"
      },
      "source": [
        "from typing import Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6ZcH5CzGKqT"
      },
      "source": [
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    experiment_name: str\n",
        "    debug: bool = False\n",
        "    fold: int = 0\n",
        "\n",
        "    nlp_model_name: str = \"roberta-large\"\n",
        "    linear_dim: int = 128\n",
        "    # dropout: float = 0\n",
        "    # dropout_stack: float = 0.2\n",
        "    batch_size: int = 6\n",
        "\n",
        "    lr_bert: float = 1e-5\n",
        "    lr_fc: float = 1e-4\n",
        "    scheduler_params = {\"num_warmup_steps\": batch_size*100,\n",
        "                        \"num_training_steps\": batch_size*3000}\n",
        "    if debug:\n",
        "        epochs: int = 2\n",
        "    else:\n",
        "        epochs: int = 15\n",
        "\n",
        "    optimizer: Any = AdamW\n",
        "    weight_decay = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A5BqDm102J5"
      },
      "source": [
        "def configure_optimizers(self):\n",
        "    optimizer =  transformers.AdamW(\n",
        "        params=[{\"params\": self.bert.parameters(), \"lr\": config.lr_bert},\n",
        "                {\"params\": self.linear.parameters(), \"lr\": config.lr_fc}],\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, **self.config.scheduler_params)\n",
        "    return [optimizer], [scheduler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jso8FlnY02CG"
      },
      "source": [
        "model, tokenizer = Auto_NLP_model(v).loads()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvHTjFqC9iyW"
      },
      "source": [
        "first part we see lr is differen for last layer and model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxV1xtIL7u7M"
      },
      "source": [
        "```\n",
        "model.roberta all model without classifier\n",
        "model.classifier\n",
        ">>  (classifier):\n",
        "    RobertaClassificationHead(\n",
        "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
        "        (dropout): Dropout(p=0.1, inplace=False)\n",
        "        (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
        "\n",
        "-----\n",
        "Fold: 1, --- ((2267, 8), (567, 8))\n",
        "tr_loader len = 378 how get >> len data = 2267 / batch = 6\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5eCkU9wBIBm"
      },
      "source": [
        "2267/6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj4tj_tc797R"
      },
      "source": [
        "list(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7HAqOHN7ekd"
      },
      "source": [
        "model.classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzryLQtK7Y6Z"
      },
      "source": [
        "steps = len(tr_loader) * EPOCH\n",
        "optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=True)\n",
        "steps = len(tr_loader) * EPOCH\n",
        "# steps = len(tr_loader)/BATCH * EPOCH\n",
        "lin_schedule = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=1,\n",
        "    num_training_steps=steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVSAR6buA1Jl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}