{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "work_on_error.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Mkusuxvxk7LrYXbpX_GBImtIi0n2BqCC",
      "authorship_tag": "ABX9TyNOv7q2eL23t3cLFDwjq3S9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqTeSQ9XezJ"
      },
      "source": [
        "## Description\n",
        "\n",
        "I wants understand why my train is different on other people where i wrong?\n",
        "I train default how i train another model detection and convolution but i see in kernels in kaggle people use another techicks to train nlp model and i can't understand why and i want test it.\n",
        "\n",
        "Links to use:\n",
        "- [22nd solution](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257302)\n",
        "\n",
        "- [gtihub link](https://github.com/kurupical/commonlit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efA57ChrZV1l"
      },
      "source": [
        "## default or not changed loads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KsrTcKtZTwC"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install transformers\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJZ_KqlNZcqQ"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import  DataLoader, Dataset\n",
        "import transformers\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "path_tr = '/content/drive/MyDrive/CommonLit/input/train.csv'\n",
        "path_test = '/content/drive/MyDrive/CommonLit/input/test.csv'\n",
        "path_sub = '/content/drive/MyDrive/CommonLit/input/sample_submission.csv'\n",
        "\n",
        "\n",
        "SEED =13\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def make_folds(data: pd.DataFrame, split: int = 5):\n",
        "    data['kfold'] = -1\n",
        "    data = data.sample(frac =1).reset_index(drop=True)\n",
        "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
        "    data.loc[:, 'bins'] = pd.cut(\n",
        "        data['target'], bins = num_bins, labels = False\n",
        "    )\n",
        "    kf = model_selection.StratifiedKFold(n_splits=split)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z]+', ' ',str(txt).lower())\n",
        "\n",
        "\n",
        "df = pd.read_csv(path_tr)\n",
        "df['txt'] = df['excerpt']#.apply(lambda x: clean_text(x))\n",
        "df_folds = make_folds(df, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2h9W_fyZhRG"
      },
      "source": [
        "class CL_Dataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        token,\n",
        "        max_len: int = 256,\n",
        "        test: bool = False\n",
        "        ) -> dict:\n",
        "        self.data = data \n",
        "        self.max_len = max_len\n",
        "        self.test = test\n",
        "        self.token = token\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def  __getitem__(self, idx: int):\n",
        "        text = self.data.txt.iloc[idx]\n",
        "        encode = self.token(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True,            \n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False  \n",
        "            )\n",
        "        if self.test:\n",
        "            target = 0\n",
        "        else:\n",
        "            target = self.data.target.iloc[idx]\n",
        "\n",
        "        ids = encode[\"input_ids\"]\n",
        "        mask = encode[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            'target': torch.tensor(target, dtype = torch.float)  \n",
        "        }"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFTGY6cBagJZ"
      },
      "source": [
        "def loss_fn(output,target):\n",
        "    return torch.sqrt(nn.MSELoss()(output,target))\n",
        "\n",
        "\n",
        "def viz_curve(data: dict, title: str) -> None:\n",
        "    epochs = list(range(1, EPOCH + 1))\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        specs=[\n",
        "               [{\"colspan\": 2}, None],\n",
        "               [{}, {}],\n",
        "               ],\n",
        "               vertical_spacing=0.09,\n",
        "               subplot_titles=('Loss Curve',  'RMSE', 'LR')\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['train_loss'],\n",
        "            mode='lines+markers',\n",
        "            name='Train Loss'),\n",
        "            row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['valid_loss'],\n",
        "            mode='lines+markers',\n",
        "            name='Valid Loss'),\n",
        "            row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['valid_rms'],\n",
        "            mode='lines+markers',\n",
        "            name='RMSE'),     \n",
        "            row=2, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=epochs,\n",
        "            y=data['lr'],\n",
        "            mode='lines+markers',\n",
        "            name='LR'), \n",
        "            row=2, col=2\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        height=600,\n",
        "        width=600,\n",
        "        showlegend=False,\n",
        "        title = title,\n",
        "        margin=dict(l=10, r=10, t=30, b=20),                     \n",
        "        template=\"plotly_dark\"    \n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def update_result(\n",
        "    data: dict,\n",
        "    predict: torch.tensor,\n",
        "    target:torch.tensor\n",
        ") -> None:\n",
        "    loss = loss_fn(predict.squeeze(-1), target)\n",
        "    data['losses'].append(loss.detach().cpu().numpy())\n",
        "    data['all_pred'].append(predict.squeeze(-1).detach().cpu().numpy())\n",
        "    data['all_target'].append(target.detach().cpu().numpy())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train(\n",
        "    model:nn.Module, \n",
        "    loader: DataLoader,\n",
        "    optimizer: transformers.AdamW,\n",
        "    schedule: transformers.get_linear_schedule_with_warmup,\n",
        "    batch: int,\n",
        "    max_lenght: int\n",
        ") -> list:\n",
        "    model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    results = defaultdict(list)\n",
        "    for input in loader:\n",
        "        optimizer.zero_grad()\n",
        "        target = input['target'].to(device)\n",
        "        batch = {k:v.to(device) for k,v in input.items() if k != 'target'}   \n",
        "        out = model(**batch)\n",
        "        out = out.logits # BertForSequenceClassification\n",
        "        loss = update_result(results, out, target)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "        optimizer.step()    \n",
        "        schedule.step()\n",
        "    losses = np.mean(results['losses'])\n",
        "    allt = np.concatenate(results['all_target'])\n",
        "    allp = np.concatenate(results['all_pred'])\n",
        "    rmse = np.sqrt(mean_squared_error(allt, allp))  \n",
        "    return losses, rmse\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model:nn.Module, \n",
        "    loader: DataLoader, \n",
        "    batch: int,\n",
        "    max_lenght: int\n",
        ") -> list:\n",
        "    model.eval()\n",
        "    results = defaultdict(list)\n",
        "    for input in loader:  \n",
        "        target = input['target'].to(device)\n",
        "        batch = {k:v.to(device) for k,v in input.items() if k != 'target'} \n",
        "        out = model(**batch)\n",
        "        out = out.logits # BertForSequenceClassification\n",
        "        _ = update_result(results, out, target)\n",
        "    losses = np.mean(results['losses'])\n",
        "    allt = np.concatenate(results['all_target'])\n",
        "    allp = np.concatenate(results['all_pred'])\n",
        "    rmse = np.sqrt(mean_squared_error(allt, allp)) \n",
        "    return losses, rmse\n",
        "\n",
        "\n",
        "def showtime(model:nn.Module, data: pd.DataFrame, tokenizer:transformers.AutoTokenizer, fold: int) -> dict:    \n",
        "    history = defaultdict(list)\n",
        "    model = model.to(device)\n",
        "    ttr = data[data.kfold != fold].reset_index(drop=True)\n",
        "    vvl = data[data.kfold == fold].reset_index(drop=True)\n",
        "    print(f'Fold: {fold + 1}, --- {ttr.shape, vvl.shape}')\n",
        "\n",
        "    tr = CL_Dataset(ttr,tokenizer, MAX_LEN)\n",
        "    vl = CL_Dataset(vvl,tokenizer, MAX_LEN)\n",
        "    tr_loader = DataLoader(\n",
        "        tr,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=True\n",
        "    )\n",
        "    vl_loader = DataLoader(\n",
        "        vl,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=False\n",
        "    )\n",
        "    optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=True)\n",
        "    steps = len(tr_loader) * EPOCH\n",
        "    # steps = len(tr_loader)/BATCH * EPOCH\n",
        "    lin_schedule = transformers.get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=1,\n",
        "        num_training_steps=steps\n",
        "    )\n",
        "    best_rmse = np.inf\n",
        "    for epoch in tqdm(range(EPOCH)):\n",
        "        tr_loss, tr_rmse = train(model, tr_loader, optimizer, lin_schedule, BATCH, MAX_LEN)\n",
        "        vl_loss, vl_rmse = validate(model, vl_loader, BATCH, MAX_LEN)\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['valid_loss'].append(vl_loss)\n",
        "        history['valid_rms'].append(vl_rmse)\n",
        "        history['lr'].append(lr)\n",
        "        print(f'Epoch: {epoch}, lr: {lr}, train rmse: {tr_rmse}, vl rmse: {vl_rmse}, vl loss: {vl_loss}')\n",
        "        if vl_rmse < best_rmse:\n",
        "            print(f'Save rmse: {vl_rmse}')\n",
        "            torch.save(model.state_dict(), f'{MODEL}_model_{fold}.pth')\n",
        "            best_rmse = vl_rmse\n",
        "    return history"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7sEEGipHsfF"
      },
      "source": [
        "class Auto_NLP_model:\n",
        "    def __init__(self, name: str, n_labels: int = 1):\n",
        "        print(f'Auto NLP init model : {name}')\n",
        "        self.name = name\n",
        "        self.n_labels=  n_labels        \n",
        "    \n",
        "    def loads(self):\n",
        "        print('Loading configuraiton...')\n",
        "        model_config = transformers.AutoConfig.from_pretrained(\n",
        "                pretrained_model_name_or_path=self.name,\n",
        "                num_labels=self.n_labels)\n",
        "        print('Loading tokenizer...')\n",
        "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "                pretrained_model_name_or_path=self.name)\n",
        "        print('Loading model...')\n",
        "        model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name_or_path=self.name,\n",
        "                config=model_config)\n",
        "        return model, tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnrkOLV73K4h"
      },
      "source": [
        "\"\"\"\n",
        "https://huggingface.co/transformers/pretrained_models.html\n",
        "https://huggingface.co/transformers/model_doc/auto.html\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "m = {\n",
        "    # 'xlm-roberta':'xlm-roberta-base',\n",
        "    # 'deberta': 'microsoft/deberta-base',\n",
        "    # 'xlm':'xlm-mlm-ende-1024',\n",
        "    # 'camembert': 'camembert-base',\n",
        "    # 'mbart': 'facebook/mbart-large-cc25', #memory batch 4 not ends error\n",
        "    'roberta': 'roberta-large'\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qilBpxo_9ude"
      },
      "source": [
        "EPOCH = 15\n",
        "BATCH = 6\n",
        "MAX_LEN = 256\n",
        "\n",
        "for k, v in m.items():\n",
        "    print(k)\n",
        "    model, tokenizer = Auto_NLP_model(v).loads()\n",
        "    MODEL = model.__class__.__name__\n",
        "    fold = 0\n",
        "    title = f'Model: {k} Fold: {fold + 1}'\n",
        "    history = showtime(model, df_folds, tokenizer, fold)\n",
        "    viz_curve(history, title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wslT6E7nbtuG"
      },
      "source": [
        "### rsults:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pgWHou-08Sq"
      },
      "source": [
        "results y model:\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "Fold: 1, --- ((2267, 8), (567, 8))\n",
        "  0%|          | 0/15 [00:00<?, ?it/s]Epoch: 0, lr: 1.866995942847063e-05, train rmse: 0.771242618560791, vl rmse: 0.5320066809654236, vl loss: 0.5065193772315979\n",
        "Save rmse: 0.5320066809654236\n",
        " 13%|█▎        | 2/15 [13:49<1:30:08, 416.03s/it]Epoch: 1, lr: 1.7336390897865588e-05, train rmse: 0.5480484366416931, vl rmse: 0.7261704802513123, vl loss: 0.7056165933609009\n",
        " 20%|██        | 3/15 [20:51<1:23:43, 418.63s/it]Epoch: 2, lr: 1.6002822367260542e-05, train rmse: 0.45481783151626587, vl rmse: 0.5933656096458435, vl loss: 0.5674742460250854\n",
        " 27%|██▋       | 4/15 [27:52<1:16:56, 419.67s/it]Epoch: 3, lr: 1.4669253836655498e-05, train rmse: 0.3679790794849396, vl rmse: 0.6310817003250122, vl loss: 0.6084640622138977\n",
        " 33%|███▎      | 5/15 [34:54<1:10:04, 420.41s/it]Epoch: 4, lr: 1.333568530605045e-05, train rmse: 0.3390931189060211, vl rmse: 0.7398288249969482, vl loss: 0.721051037311554\n",
        " 40%|████      | 6/15 [41:56<1:03:08, 420.91s/it]Epoch: 5, lr: 1.2002116775445406e-05, train rmse: 0.2956746220588684, vl rmse: 0.5392723083496094, vl loss: 0.5154401659965515\n",
        " 47%|████▋     | 7/15 [48:58<56:11, 421.40s/it]  Epoch: 6, lr: 1.066854824484036e-05, train rmse: 0.2643064260482788, vl rmse: 0.5453745126724243, vl loss: 0.5227868556976318\n",
        " 53%|█████▎    | 8/15 [56:01<49:11, 421.65s/it]Epoch: 7, lr: 9.334979714235315e-06, train rmse: 0.25952309370040894, vl rmse: 0.6315166354179382, vl loss: 0.6088392734527588\n",
        " 60%|██████    | 9/15 [1:03:03<42:11, 421.85s/it]Epoch: 8, lr: 8.001411183630271e-06, train rmse: 0.2210676074028015, vl rmse: 0.659906804561615, vl loss: 0.6411230564117432\n",
        " 67%|██████▋   | 10/15 [1:10:05<35:10, 422.07s/it]Epoch: 9, lr: 6.667842653025225e-06, train rmse: 0.21520286798477173, vl rmse: 0.5818750262260437, vl loss: 0.5584777593612671\n",
        "Epoch: 10, lr: 5.33427412242018e-06, train rmse: 0.20328344404697418, vl rmse: 0.5239043235778809, vl loss: 0.4989898204803467\n",
        "Save rmse: 0.5239043235778809\n",
        " 80%|████████  | 12/15 [1:24:15<21:09, 423.21s/it]Epoch: 11, lr: 4.0007055918151355e-06, train rmse: 0.18055203557014465, vl rmse: 0.5367754697799683, vl loss: 0.5130714178085327\n",
        " 87%|████████▋ | 13/15 [1:31:17<14:05, 422.95s/it]Epoch: 12, lr: 2.66713706121009e-06, train rmse: 0.1665629744529724, vl rmse: 0.5970629453659058, vl loss: 0.5752689838409424\n",
        " 93%|█████████▎| 14/15 [1:38:19<07:02, 422.65s/it]Epoch: 13, lr: 1.333568530605045e-06, train rmse: 0.15745727717876434, vl rmse: 0.5785849094390869, vl loss: 0.5563144683837891\n",
        "100%|██████████| 15/15 [1:45:21<00:00, 421.43s/it]Epoch: 14, lr: 0.0, train rmse: 0.14649353921413422, vl rmse: 0.5729362964630127, vl loss: 0.5512558817863464\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8pEUOfA1YgU"
      },
      "source": [
        "## adds something new"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6ZcH5CzGKqT"
      },
      "source": [
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    experiment_name: str\n",
        "    debug: bool = False\n",
        "    fold: int = 0\n",
        "\n",
        "    nlp_model_name: str = \"roberta-large\"\n",
        "    linear_dim: int = 128\n",
        "    # dropout: float = 0\n",
        "    # dropout_stack: float = 0.2\n",
        "    batch_size: int = 8\n",
        "\n",
        "    lr_bert: float = 1e-5\n",
        "    lr_fc: float = 1e-4\n",
        "    scheduler_params = {\"num_warmup_steps\": 16*100,\n",
        "                        \"num_training_steps\": 16*3000}\n",
        "    if debug:\n",
        "        epochs: int = 2\n",
        "    else:\n",
        "        epochs: int = 15\n",
        "\n",
        "    optimizer: Any = AdamW\n",
        "    weight_decay = 0.1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A5BqDm102J5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jso8FlnY02CG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}