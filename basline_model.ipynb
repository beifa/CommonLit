{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basline_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Qh6cVMR7m27dXJo-1VMxoWiEqQU4Hq68",
      "authorship_tag": "ABX9TyPyHtW2W2DaXWjNAGOzWZ2F"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KsrTcKtZTwC"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install transformers\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "TJZ_KqlNZcqQ",
        "outputId": "870345c6-4d37-4d56-83dd-58e3e2ee6be1"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import  DataLoader, Dataset\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "path_tr = '/content/drive/MyDrive/CommonLit/input/train.csv'\n",
        "path_test = '/content/drive/MyDrive/CommonLit/input/test.csv'\n",
        "path_sub = '/content/drive/MyDrive/CommonLit/input/sample_submission.csv'\n",
        "\n",
        "df = pd.read_csv(path_tr)\n",
        "\n",
        "SEED =13\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z]+', ' ',str(txt).lower())\n",
        "df['txt'] = df['excerpt'].apply(lambda x: clean_text(x))\n",
        "\n",
        "df.head(2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "      <td>when the young people returned to the ballroom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "      <td>all through dinner time mrs fayre was somewhat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                                txt\n",
              "0  c12129c31  ...  when the young people returned to the ballroom...\n",
              "1  85aa80a4c  ...  all through dinner time mrs fayre was somewhat...\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TrBVs4DZveO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2h9W_fyZhRG"
      },
      "source": [
        "class CL_Dataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        max_len: int = 256,\n",
        "        test: bool = False\n",
        "        ) -> dict:\n",
        "        self.data = data \n",
        "        self.max_len = max_len\n",
        "        self.test = test\n",
        "        self.token = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def  __getitem__(self, idx: int):\n",
        "        text = self.data.txt.iloc[idx]\n",
        "        encode = self.token.encode_plus(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True,            \n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt'\n",
        "            )\n",
        "        if self.test:\n",
        "            target = 0\n",
        "        else:\n",
        "            target = self.data.target.iloc[idx]                    \n",
        "        return {\n",
        "                'input_ids': encode['input_ids'],\n",
        "                'attention_mask': encode['attention_mask'],\n",
        "                'target': torch.tensor(target, dtype = torch.float)  \n",
        "                }\n",
        "\n",
        "class CL_model(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_out:int = 1):\n",
        "        super(CL_model, self).__init__()\n",
        "        self.dim_out = dim_out\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.drop = nn.Dropout(p = 0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, self.dim_out)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out_model = self.bert(input_ids, attention_mask)\n",
        "        d1 = self.drop(out_model['pooler_output'])\n",
        "        out = self.out(d1)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFTGY6cBagJZ"
      },
      "source": [
        "def loss_fn(output,target):\n",
        "    return torch.sqrt(nn.MSELoss()(output,target))\n",
        "\n",
        "\n",
        "def train(\n",
        "    model:nn.Module, \n",
        "    loader: DataLoader,\n",
        "    optimizer: transformers.AdamW,\n",
        "    schedule: transformers.get_linear_schedule_with_warmup,\n",
        "    batch: int,\n",
        "    max_lenght: int\n",
        ") -> list:\n",
        "    model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    all_pred, all_target, losses = [], [], []\n",
        "    for input in loader:\n",
        "        optimizer.zero_grad()\n",
        "        ii = input['input_ids'].squeeze().to(device) # view(batch, max_lenght).to(device)\n",
        "        am = input['attention_mask'].squeeze().to(device) # .view(batch, max_lenght).to(device)\n",
        "        target = input['target'].to(device)\n",
        "        out = model(input_ids =ii, attention_mask = am)\n",
        "        # out = out.logits # BertForSequenceClassification\n",
        "        loss = loss_fn(out.squeeze(-1), target)\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "        all_pred.append(out.squeeze(-1).detach().cpu().numpy())\n",
        "        all_target.append(target.detach().cpu().numpy())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "        optimizer.step()    \n",
        "        schedule.step()\n",
        "    losses = np.mean(losses)\n",
        "    allt = np.concatenate(all_target)\n",
        "    allp = np.concatenate(all_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(allt, allp))  \n",
        "    return losses, rmse\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model:nn.Module, \n",
        "    loader: DataLoader, \n",
        "    batch: int,\n",
        "    max_lenght: int\n",
        ") -> list:\n",
        "    model.eval()\n",
        "    all_pred, all_target, losses = [], [], []\n",
        "    for input in loader:\n",
        "        ii = input['input_ids'].squeeze().to(device) # view(batch, max_lenght).to(device)\n",
        "        am = input['attention_mask'].squeeze().to(device) # .view(batch, max_lenght).to(device)\n",
        "        target = input['target'].to(device)\n",
        "        out = model(input_ids =ii, attention_mask = am)\n",
        "        # out = out.logits # BertForSequenceClassification\n",
        "        loss = loss_fn(out.squeeze(-1), target)\n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "        all_pred.append(out.squeeze(-1).detach().cpu().numpy())\n",
        "        all_target.append(target.detach().cpu().numpy())  \n",
        "    losses = np.mean(losses)\n",
        "    allt = np.concatenate(all_target)\n",
        "    allp = np.concatenate(all_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(allt, allp)) \n",
        "    return losses, rmse\n",
        "\n",
        "\n",
        "def showtime(model:nn.Module, data: pd.DataFrame) -> dict:\n",
        "    \n",
        "    history = defaultdict(list)\n",
        "    model = model.to(device)\n",
        "    X_train, X_test = train_test_split(\n",
        "        data,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED\n",
        "    )# ((2267, 6), (567, 6))\n",
        "    tr = CL_Dataset(X_train, MAX_LEN)\n",
        "    vl = CL_Dataset(X_test,  MAX_LEN)\n",
        "    tr_loader = DataLoader(\n",
        "        tr,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=True\n",
        "    )\n",
        "    vl_loader = DataLoader(\n",
        "        vl,\n",
        "        batch_size=BATCH,\n",
        "        shuffle=False\n",
        "    )\n",
        "    optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=True)\n",
        "    steps = len(tr_loader) * EPOCH\n",
        "    lin_schedule = transformers.get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=1,\n",
        "        num_training_steps=steps\n",
        "    )\n",
        "    fold = 0\n",
        "    best_rmse = 100\n",
        "    for epoch in tqdm(range(EPOCH)):\n",
        "        tr_loss, tr_rmse = train(model, tr_loader, optimizer, lin_schedule, BATCH, MAX_LEN)\n",
        "        vl_loss, vl_rmse = validate(model, vl_loader, BATCH, MAX_LEN)\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['valid_loss'].append(vl_loss)\n",
        "        history['valid_rms'].append(vl_rmse)\n",
        "        history['lr'].append(lr)\n",
        "        print(f'Epoch: {epoch}, lr: {lr}, train rmse: {tr_rmse}, vl rmse: {vl_rmse}, vl loss: {vl_loss}')\n",
        "        if vl_rmse < best_rmse:\n",
        "            print(f'Save rmse: {vl_rmse}')\n",
        "            torch.save(model.state_dict(), f'{MODEL}_model_{fold}_{vl_rmse}.pth')\n",
        "    return history"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pxC2vLvfohS"
      },
      "source": [
        "EPOCH = 10\n",
        "BATCH = 8\n",
        "MAX_LEN = 314\n",
        "model = CL_model()\n",
        "# model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "MODEL = model.__class__.__name__\n",
        "showtime(model, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1hiqGsa393P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNGYHB2U_awV"
      },
      "source": [
        "stop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utgS5obtLY72"
      },
      "source": [
        "#inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5EMQKbknWaO"
      },
      "source": [
        "import gc\n",
        "_ = gc.collect()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llydl2YFpbHR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "4de117f9-067f-4309-845c-5fed5abbd774"
      },
      "source": [
        "test = pd.read_csv(path_test)\n",
        "test.head(1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c0f722661</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                            excerpt\n",
              "0  c0f722661  ...  My hope lay in Jack's promise that he would ke...\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXPO1Cb_pgc9"
      },
      "source": [
        "@torch.no_grad()\n",
        "def inference(model:nn.Module, data: pd.DataFrame):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    test = CL_Dataset(data, MAX_LEN, True)\n",
        "    test_loader = DataLoader(\n",
        "        test,\n",
        "        batch_size=1,\n",
        "        shuffle=False\n",
        "    )\n",
        "    all_pred = []\n",
        "    for input in tqdm(test_loader):\n",
        "        ii = input['input_ids'].view(1, MAX_LEN).to(device)\n",
        "        am = input['attention_mask'].view(1, MAX_LEN).to(device)\n",
        "        out = model(input_ids =ii, attention_mask = am)\n",
        "        all_pred.append(out.squeeze(-1).detach().cpu().numpy())\n",
        "    return np.concatenate(all_pred)\n",
        "    \n",
        "\n",
        "model = CL_model()\n",
        "model.load_state_dict(torch.load('/content/CL_model_model_0_0.5428774356842041.pth'))\n",
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z]+', ' ',str(txt).lower())\n",
        "\n",
        "\n",
        "test['txt'] = test['excerpt'].apply(lambda x: clean_text(x))\n",
        "pred = inference(model, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ5H8671pgjl"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHydG5rVLVdh"
      },
      "source": [
        "#save to load local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9eWB0vgKiU6"
      },
      "source": [
        "# save token\n",
        "token = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "token.save_pretrained(\"./model_uncase_bert\")\n",
        "#save model\n",
        "bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "bert.save_pretrained(\"./model_uncase_bert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuT0ne_9k16Y"
      },
      "source": [
        "model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "token = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "token.save_pretrained(\"./model_uncase_bert\")\n",
        "#save model\n",
        "model.save_pretrained(\"./model_uncase_bert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dry3oHnOwGoo"
      },
      "source": [
        "# Experements\n",
        "EPOCH = 10, BATCH = 8\n",
        "\n",
        "- (?) clear_txt, max_length = 314 = score - 0.56\n",
        "- (best train 0.5789) clear_txt, max_length = 256 = score - 0.608\n",
        "- (best train 0.5466) clear_txt, max_length = 255 = score - 0.574\n",
        "- (best train 0.5595) clear_txt, max_length = 356  = score -0.583\n",
        "- (best train 0.5294) clear_txt, max_length = 314  = score -0.534\n",
        "- (best train 0.5572 ) clear_txt, max_length = 192  = score -0.582\n",
        "\n",
        "change bert to clasifir\n",
        "- (best train 0.5596) clear_txt, max_length = 255  = score -0.593\n",
        "\n",
        "autonlp\n",
        "- (best train 0.6111) clear_txt, max_length = 255  = score -0.605\n",
        "\n",
        "t5 -fast\n",
        "- (best train 0.6111) clear_txt, max_length = 255  = score -0.605\n",
        "\n",
        "batch up 8 --> 16\n",
        "- clear_txt, max_length= score=\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_2ESZ1JoQre"
      },
      "source": [
        "make folds abusheck\n",
        "make learn by folds in kaggle use gpu maybe use large modle\n",
        "use two accounts in kaggle\n",
        "\n",
        "make plot result train\n",
        "\n",
        "make auto nlp\n",
        "check score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}